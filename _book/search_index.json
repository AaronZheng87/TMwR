[
["index.html", "Tidy Modeling with R Hello World", " Tidy Modeling with R Max Kuhn 2019-12-10 Hello World This is the website for Tidy Modeling with R. Its purpose is to be a guide to using a new collection of software in the R programming language that enable model building. There are few goals, depending on your background. First, if you are new to modeling and R, we hope to provide an introduction on how to use software to create models. The focus will be on a dialect of R called the tidyverse that is designed to be a better interface for common tasks using R. If you’ve never heard of the tidyverse, there is a chapter that provides a solid introduction. The second (and primary) goal is to demonstrate how the tidyverse can be used to produce high quality models. The tools used to do this are referred to as the tidymodels packages. The third goal is to use the tidymodels packages to encourage good methodology and statistical practice. Many models, especially more complex predictive or machine learning models, can be created to work very well on the data at hand but may fail when exposed to new data. Often, this issue is due to poor choices that were made during the development and/or selection of the models. Whenever possible, our software attempts to prevent this from occurring but common pitfalls are discussed in the course of describing and demonstrating the software. This book is not intended to be a reference on different types of models. We suggest other resources to learn the nuances of models. A general source for information about the most common type of model, the linear model, we suggest Fox (2008). Another excellent resource for investigating and analyzing data is Wickham and Grolemund (2016). For predictive models, Kuhn and Johnson (2013) is a good resource. For pure machine learning methods, Goodfellow, Bengio, and Courville (2016) is an excellent (but formal) source of information. In some cases, we describe some models that are used in this text but in a way that is less mathematical (and hopefully more intuitive). We do not assume that readers will have had extensive experience in model building and statistics. Some statistical knowledge is required, such as: random sampling, variance, correlation, basic linear regression, and other topics that are usually found in a basic undergraduate statistics or data analysis course. This website is free to use, and is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 3.0 License. The sources used to create the book can be found at github.com/topepo/TMwR. We use the bookdown package to create the website (Xie 2016). One reason that we chose this license and this technology for the book is so that we can make it completely reproducible; all of the code and data used to create it are free and publicly available. Tidy Modeling with R is currently a work in progress. As we create it, this website is updated. Be aware that, until it is finalized, the content and/or structure of the book may change. This openness also allows users to contribute if they wish. Most often, this comes in the form of correcting types, grammar, and other aspects of our work that could use improvement. Instructions for making contributions can be found in the contributing.md file. Also, be aware that this effort has a code of conduct, which can be found at code_of_conduct.md. In terms of software lifecycle, the tidymodels packages are fairly young. We will do our best to maintain backwards compatibility and, at the completion of this work, will archive the specific versions of software that were used to produce it. The primary packages, and their versions, used to create this website are: #&gt; ─ Session info ─────────────────────────────────────────────────────────────── #&gt; setting value #&gt; version R version 3.6.1 (2019-07-05) #&gt; os macOS Mojave 10.14.6 #&gt; system x86_64, darwin15.6.0 #&gt; ui X11 #&gt; language (EN) #&gt; collate en_US.UTF-8 #&gt; ctype en_US.UTF-8 #&gt; tz America/New_York #&gt; date 2019-12-10 #&gt; #&gt; ─ Packages ─────────────────────────────────────────────────────────────────── #&gt; package * version date lib source #&gt; AmesHousing * 0.0.3 2017-12-17 [1] CRAN (R 3.6.0) #&gt; bookdown * 0.14 2019-10-01 [1] CRAN (R 3.6.0) #&gt; broom * 0.5.2 2019-04-07 [1] CRAN (R 3.6.0) #&gt; dials * 0.0.4 2019-12-02 [1] CRAN (R 3.6.1) #&gt; discrim * 0.0.1 2019-10-10 [1] local #&gt; dplyr * 0.8.3 2019-07-04 [1] CRAN (R 3.6.0) #&gt; ggplot2 * 3.2.1.9000 2019-12-06 [1] local #&gt; infer * 0.5.1 2019-11-19 [1] CRAN (R 3.6.0) #&gt; parsnip * 0.0.4.9000 2019-12-04 [1] local #&gt; purrr * 0.3.3 2019-10-18 [1] CRAN (R 3.6.0) #&gt; recipes * 0.1.7.9002 2019-12-10 [1] local #&gt; rlang 0.4.2.9000 2019-12-01 [1] Github (r-lib/rlang@1be25e7) #&gt; rsample * 0.0.5 2019-07-12 [1] CRAN (R 3.6.0) #&gt; tibble * 2.99.99.9010 2019-12-06 [1] Github (tidyverse/tibble@f4365f7) #&gt; tune * 0.0.0.9004 2019-12-09 [1] local #&gt; workflows * 0.0.0.9002 2019-11-29 [1] local #&gt; yardstick * 0.0.4 2019-08-26 [1] CRAN (R 3.6.0) #&gt; #&gt; [1] /Library/Frameworks/R.framework/Versions/3.6/Resources/library pandoc is also instrumental in creating this work. The version used here is 2.3.1. References "],
["introduction.html", "1 Introduction 1.1 Let’s talk about models 1.2 What does it mean to be “tidy” 1.3 Outline of future chapters", " 1 Introduction Models are mathematical tools that create equations that are intended to mimic the data given to them. These equations can be used for various purposes, such as: predicting future events, determining if there is a different between two groups, as an aid to a map-based visualization, discovering novel patterns in the data that could be further investigated, and so on. Their utility hinges on their ability to be reductive; the primary influences in the data can be captured mathematically in a way that is useful. In the last 20 years, mathematical models have become ubiquitous in our daily lives, in both obvious and subtle ways. A typical day for many people might involve checking the weather to see when a good time would be to walk the dog, ordering a product from a website, typing (and autocorrecting) a text message to a friend, and checking email. In each of these instances, there is a good chance that some type of model was used in an assistive way. In some cases, the contribution of the model might be easily perceived (“You might also be interested in purchasing product X”) while in other cases the impact was the absence of something (e.g., spam email). Models are used to choose clothing that a customer might like, a molecule that should be evaluated as a drug candidate, and might even be the mechanism that a nefarious company uses avoid the discovery of cars that over-pollute. For better or worse, models are here to stay. Two reasons that models permeate our lives are that software exists that facilitates their creation and that data has become more easily captured and accessible. In regard to software, it is obviously critical that software produces the correct equations that represent the data. For the most part, determining mathematical correctness is possible. However, the creation of models hinges on a few non-technical aspects. First, it is important that it is easy to operate the software in a proper way. For example, the user interface should not be so arcane that the user would not know that they have inappropriately specified the wrong information. As an analogy, one might have a high quality kitchen measuring cup capable of great precision but if the chef adds a cup of salt instead of a cup of sugar, the results would be unpalatable. As a specific example of this issue, Baggerly and Coombes (2009) report myriad problems in the data analysis in a high profile computational biology publication. One of the issues was related to how the users were required to add the names of the model inputs. The user-interface of the software was poor enough that it was easy to offset the column names of the data from the actual data columns. In the analysis of the data, this resulted in the wrong genes being identified as important for treating cancer patients. This, and many other issues, led to the stoppage of numerous clinical trials (Carlson 2012). If we are to expect high quality models, it is important that the software facilitate proper usage. TODO add a quote about the pit of success (is Brad Abrams blog the original reference for this term?) The second important non-technical aspect of model building is related to scientific methodology. For models that are used to make complex predictions, it can be easy to unknowingly commit errors related to logical fallacies or inappropriate assumptions. Many machine learning models are so adept at finding patterns, they can effortlessly find empirical patterns in the data that fail to reproduce later. Some of these types of methodological errors are insidious in that the issue might be undetectable until a later time when new data that contain the true result are obtained. In short, as our models become more powerful and complex it has also become easier to commit latent errors. This relates to software. Whenever possible, the software should be able to protect users from committing such mistakes. Here, software should make it easy for users to do the right thing. These two aspects of model creation are crucial. Since tools for creating models are easily obtained and models can have such a profound impact, many more people are creating them. In terms of technical expertice and training, their backgrounds will vary. It is important that their tools be robust to the experience of the user. On one had, they tools should be powerful enough to create high-performance models but, one the other hand, should be easy to use in an appropriate way. This book desribes a suite of software that can can create different types of models. The software has been designed with these non-technical asepcts in mind. The software is based on the R programming language (R Core Team 2014). R has been designed especially for data analysis and modeling. It is based on the S language which was created in the 1970’s to “to turn ideas into software, quickly and faithfully” (Chambers 1998) R has a vast ecosystem of packages; these are mostly user-contributed modules that focus on a specific theme, such as modeling, visualization, and so on. One collection of packages is called the tidyverse (Wickham et al. 2019). The tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures. Several of these design philosophies are directly related to the aspects of software described above. Within the tidyverse, there is a set of pacakges specifically focused on modeling and these are usually refered to as the tidymodels packages. 1.1 Let’s talk about models 1.1.1 Types of Models descriptive, predictive, inferential regression, classification 1.1.2 Some terminology supervise, unsupervised types of predictors 1.1.3 Where does modeling fit into the scientific process? Figure 1.1: The data science process (from R for Data Science). 1.1.4 Modeling is a process, not a single activity Figure 1.2: A schematic for the typical modeling process. 1.2 What does it mean to be “tidy” 1.3 Outline of future chapters References "],
["a-tidyverse-primer.html", "2 A tidyverse primer 2.1 Principles 2.2 Code 2.3 Why tidiness is important for modeling 2.4 Some additional tidy principals for modeling.", " 2 A tidyverse primer 2.1 Principles 2.2 Code Things that I think that we’ll need summaries of: strategies: variable specification, pipes (with data or other first arguments), conflicts and using namespaces, splicing, non-standard evaluation, tactics: select, bind_cols, tidyselect, slice, !! and !!!, ... for passing arguments, tibbles, joins, nest/unnest, group_by 2.3 Why tidiness is important for modeling 2.4 Some additional tidy principals for modeling. "],
["a-tale-of-two-models.html", "3 A tale of two models", " 3 A tale of two models (tentative title) Perhaps show an example of a predictive model and contrast it with another that is inferential. Chicago data from FES: one predictive model and one to test if there is a difference in ridership with the Bears are at home. what do we care about for each? how accurate is the inferential model? Perhaps look at the tscount package to deal with the autoregressive potential. "],
["spending-our-data.html", "4 Spending our data", " 4 Spending our data General data splitting Re-emphasize roles or different data sets and good/bad ways of doing things. Validation sets. What we do differently with a lot of data. Allude to resampling. "],
["how-good-is-our-model.html", "5 How good is our model?", " 5 How good is our model? (or how well does our model work? Superman does good; a model can work well) Measuring performance Don’t revaluate the training set Statistical significance as a measure of effectiveness. "],
["feature-engineering.html", "6 Feature engineering", " 6 Feature engineering Purpose(s) of these activites. Why do we do this? Different representations of same data Imputation; transformations; (unsup) removal; projection; encodings; "],
["a-model-workflow.html", "7 A model workflow", " 7 A model workflow aka modeling process or model pipeline How to encapsulate the pre-processing and model objects/activities Treat them as a single unit for good methodology and convenience. "],
["resampling-for-evaluating-performance.html", "8 Resampling for evaluating performance", " 8 Resampling for evaluating performance Maybe inlcude some simple examples of comparing models using resampling (perhaps go full tidyposterior?) "]
]
