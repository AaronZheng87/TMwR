# A tidyverse primer {#tidyverse-primer}

```{r tidyverse-setup, include = FALSE}
knitr::opts_chunk$set(fig.path = "figures/")
library(tidyverse)
library(lubridate)
```

The tidyverse is a collection of R packages for data analysis that are developed with common ideas and norms. From @tidyverse: 

> "At a high level, the tidyverse is a language for solving data science challenges with R code. Its primary goal is to facilitate a conversation between a human and a computer about data. Less abstractly, the tidyverse is a collection of R packages that share a high-level design philosophy and low-level grammar and data structures, so that learning one package makes it easier to learn the next."

In this chapter, these principles are briefly discussed before diving into specific examples of tidyverse syntax. 

## Principles

The full set of strategies and tactics for writing R code in the tidyverse style can be found at the website [`https://design.tidyverse.org`](https://design.tidyverse.org/). The following sections describe a few ideas and their motivation. 

**Design for humans**

The tidyverse focuses on designing R packages and functions that can be easily understood and used by a broad range of people. Historically, a substantial percentage of R users are not programmers but people who are more focused on the end results of their analyses (rather than the software complexities of their tools). As such, there should not be the expectation that users have computer science backgrounds or would be capable or writing their own R packages.

For this reason, it is critical that R code be easy to work with to accomplish your goals. Documentation, training, accessibility, and other factors play an important part in achieving this. However, if the syntax itself is difficult for people to easily comprehend, documentation is a poor solution to this problem. The software itself must be intuitive.

To contrast the tidyverse approach with more traditional R semantics, consider sorting a data frame. Using only the core language, sorting a data frame using one or more columns is achieved by reordering the rows via R's subscripting rules in conjunction with `order()` (as opposed to `sort()`). To sort the iris data by two of its columns, the call might look like:

```{r tidyverse-base-sort, eval = FALSE}
iris[order(iris$Species, iris$Sepal.Length), ]
```

While very computationally efficient, it would be difficult to argue that this is an intuitive user-interface. In `dplyr`, the `arrange()` function takes a set of variable names as input arguments directly:

```{r tidyverse-dplyr-sort, eval = FALSE}
library(dplyr)
arrange(data = iris, Species, Sepal.Length)
```

```{block, type = "rmdnote"}
Variable names above are unquoted; where many traditional R functions require a character string to specify variables, tidyverse functions take unquoted names or _selector functions_. The selectors allow for one or more  readable rules that are applied to the column names. For example, `starts_with("Sepal")` would select the first two columns of the `iris` data frame.
```

Additionally, the naming of things is crucial. If you were new to R and were writing data analysis code utilizing linear algebra, you might be stymied when searching for the function that computes the matrix inverse. Using `apropos("inv")` yields no candidates. It turns out that the function to use is `solve()` for solving systems of linear equations. For a matrix `X`, you would use `solve(X)` (with no vector for the right-hand side of the equation). This is only documented in the description in one of the arguments in the help file. In essence, you would need to know the name of the solution to be able to find the solution. 

The tidyverse approach is use function names that are explicit and descriptive instead of those that are short and implicit. There is a focus on verbs (e.g. `fit`, `arrange`, etc.) as general methods. Verb-noun pairs are particularly effective; consider `invert_matrix()` as a function name. As noted below, in the context of modeling, it is also important to avoid highly technical jargon in names (e.g. Greek letters, etc). Names should be as self-documenting as possible. 

Also, when there are similar functions in a package, the function names are designed to be optimized for tab-complete. For example, the `glue` package has a collection of functions starting with a common prefix (`glue_`) that enables users to quickly find the function that they are looking for. 


**Reuse existing data structures**

Whenever possible, functions should avoid returning a novel data structure. If the results are conducive to an existing data structure, it should be used. The reason is that it reduces the cognitive load for using the package; no additional syntax or methods are required. 

One data structure that should be used as much as possible is the data frame. Data frames enable multiple values (in rows) and the columns can represent different types of data. Tibbles, a type of data frame described below, are preferred since they contain additional properties that are helpful for data analysis. 

As an example, the `rsample` package can be used to create _resamples_ of a data set, such as cross-validation or the bootstrap (described in Chapter \@ref(resampling)). The resampling functions return a tibble of results that embed the objects that define the resampled data sets in a column called `splits`. For example, for three bootstrap samples of a data set might look like: 

```{r tidyverse-resample}
boot_samp <- rsample::bootstraps(mtcars, times = 3)
boot_samp
class(boot_samp)
```

In doing so, a variety of vector-based functions can be used with these columns, such as `vapply()` or `purrr::map()`^[If you've never seen `::` in R code before, it is a method to be explicit about what function you are calling. The value of the right-hand side is the _namespace_ where the function lives (usually a package name). The left-hand side is the function name. In cases where two packages use the same function name, this syntax will ensure that the correct function is invoked.]. This object has multiple classes but inherits some methods for data frames and tibbles. Additionally, new columns can be added to the results without affecting the class of the data. This is much easier and versatile for users to work with than new object type that does not make its data structure obvious. 

One downside to relying on common data structures is the potential loss of computational performance. In some situations, data can be encoded in specialized formats that are more efficient representations of the data. For example: 

 * In computational chemistry, the structure-data file format (SDF) is a tool to take chemical structures and encode them in a format that is computationally efficient to work with. 

 * Data that have a large number of values that are the same (such as zeros for binary data) can be stored in a _sparse matrix format_. This format can reduce the size of the data as well as enable more efficient computational techniques. 

These formats are advantageous when the problem is _small in scope_ and the potential data processing methods are well defined. However, once the constraints on the data are violated, perhaps by requiring a new type of data, then system becomes overly restrictive. For example, if a principal component analysis was needed for a sparse binary matrix, the results would no longer be sparse. Also, if no function exists to do these computations on a sparse format, the sparse matrix would need to be converted to a more conventional format before proceeding.    

One important feature in the tibble produced by `rsample` is that the `splits` column is a list. In this instance, each element of the list has the same type of object: an `rsplit` object that contains the information about which rows of `mtcars` belong in the bootstrap sample. _List columns_ can be very useful in data analysis and, as will be seem throughout this book, are very important to the tidyverse. 


**Design for the pipe and functional programming**

The `magrittr` pipe operator (`%>%`) is a tool for chaining together a sequence of R commands. To demonstrate, these commands can be used to sort a data frame then retain the first 10 rows:

```{r tidyverse-no-pipe, eval = FALSE}
small_iris <- arrange(iris, Species)
small_iris <- slice(small_iris, 1:10)

# or more compactly: 
small_iris <- slice(arrange(iris, Species), 1:10)
```

The pipe operator substitutes the value of the left-hand side of the operator as the first argument to the right-hand side: 

```{r tidyverse-pipe, eval = FALSE}
small_iris <- 
  iris %>% 
  arrange(Species) %>% 
  slice(1:10)
```

The piped version of this sequence is more readable and the readability increases as more operations are added to the sequence. This approach to programing works in this example because all of the functions that are used return a data structure (a data frame) that is the first argument to the next function. This is by design. When possible, create functions that can be incorporated into a pipeline of operations. 

If you have used `ggplot2`, this is not unlike the layering of plot aspects into a `ggplot` object with the `+` operator. To make a scatterplot with a regression line, the initial `ggplot()` call is augmented with two additional operations:

```{r tidyverse-ggplot-chain, eval = FALSE}
library(ggplot2)
ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point() + 
  geom_smooth(method = lm)
```

While similar to the `dplyr` pipeline, note that the first argument to this pipeline is a data set (`mtcars`) and that each function call returns a `ggplot` object. Not all pipelines need to keep the returned values (plot objects) the same as the initial value (a data frame). Also, using the the pipe operator with `dplyr` operations has acclimated users to expecting to return a data frame when pipelines are used; as shown with `ggplot`, this does not need to be the case. 

R has excellent tools for creating, changing, and operating functions. As such it facilitates _functional programing_. This type of computation can easily replace iterative loops under many situations, such as when a function returns a value without other side-effects (such as changing global data). 

As a simple example, suppose that there was interest in the logarithm of the ratio of the sepal length to the petal length in the iris data. For those new to R, a loop might seem reasonable:

```{r tidyverse-loop}
n <- nrow(iris)
ratios <- rep(NA_real_, n)
for (flower in 1:n) {
  ratios[flower] <- log(iris$Sepal.Length[flower]/iris$Petal.Length[flower])
}
head(ratios)
```

For those with any experience in R, there is a much simpler and faster _vectorized version_ that can be computed by:


```{r tidyverse-vectorized}
ratios <- log(iris$Sepal.Length/iris$Petal.Length)
```

However, in some cases, the element-wise operation is too complex for a vectorized solution. In this case, a good approach would be to write a function to do the computations. When we design for functional programming, it is important that the output only depends on the inputs and that the functions has no side-effects. For example, a violation of these ideas in the following function are shown with comments:

```{r tidyverse-non-functional}
compute_log_ratio <- function(sepal, petal) {
  log_base <- getOption("log_base", default = exp(1)) # gets external data
  results <- log(sepal/petal, base = log_base)
  print(mean(results))                                # prints to the console
  done <<- TRUE                                       # sets external data
  results
}
```

A better version would be

```{r tidyverse-better-function}
compute_log_ratio <- function(sepal, petal, log_base = exp(1)) {
  log(sepal/petal, base = log_base)
}
```

The `purrr` package contains tools for functional programming. Here, we will focus on the `map()` family of functions. These operate on vectors and always return the same type of output. The most basic function, `map()` always returns a list and has the basic syntax of `map(vector, function)`. For example, to take the square-root of the data: 

```{r map-basic}
map(head(iris$Sepal.Length, 3), sqrt)
``` 

There are specialized variants that return values when we know that the function generates one of the basic vector types. For example, since the square-root returns a double-precision number: 

```{r map-dbl}
map_dbl(head(iris$Sepal.Length, 3), sqrt)
``` 

There are also mapping functions that simultaneously operate across multiple vectors: 

```{r map2}
ratios <- map2_dbl(iris$Sepal.Length, iris$Petal.Length, compute_log_ratio)
head(ratios)
```

The `map()` functions also allow for temporary, anonymous functions. These are defined using the tilde character and the argument values are `.x` and `.y` for `map2()`:

```{r map2-inline}
map2_dbl(iris$Sepal.Length, iris$Petal.Length, ~ log(.x/.y)) %>% 
  head()
```

These examples have been trivial but, in later sections, are applied to more complex problems. 

The important note is that, for functional programming, functions should be defined in a manner where functions like `map()` can be used for computations. 


## Examples of tidyverse syntax

Before diving into examples, it is important to discuss how the tidyverse relies on a type of data frames called a "tibble". Tibbles have slightly different rules than basic data frames. For example, tibbles natural with  column names that are not syntactically valid variable names:

```{r tidyverse-names}
# Wants valid names:
data.frame(`variable 1` = 1:2, two = 3:4)
# But can be coerced to use them with an extra option:
df <- data.frame(`variable 1` = 1:2, two = 3:4, check.names = FALSE)
df

# But tibbles just work:
tbbl <- tibble(`variable 1` = 1:2, two = 3:4)
tbbl
```

Standard data frames enable _partial matching_ of arguments so that code that a portion of the column names still work. Tibbles prevent this from happening since it can lead to accidental errors. 

```{r tidyverse-partial, error = TRUE}
df$tw

tbbl$tw
```

Tibbles also prevent one of the most common R errors: dropping of dimensions. If a standard data frame subsets the columns down to a single column, the object is converted to a vector. Tibbles _never_ do this:

```{r tidyverse-drop}
df[, "two"]

tbbl[, "two"]
```

There are various other advantages to using tibbles instead of data frames, such as better printing and so on.  

```{r tidyverse-import-raw, include = FALSE}
url <- "chi.csv"
train_cols <- 
  cols(
    station_id = col_double(),
    stationname = col_character(),
    date = col_character(),
    daytype = col_character(),
    rides = col_double()
  )
num_combos <- 
  read_delim(url, delim = ",", col_types = train_cols) %>% 
  distinct(date, stationname) %>% 
  nrow()
```

To demonstrate some syntax, the tidyverse will be used to read in data that could be used in modeling. The data set comes from the city of Chicago's data portal and contains daily ridership data for the city's elevated train stations. The data set has columns for: the station identifier (numeric), the station name (character), the date (character in `mm/dd/yyyy` format), the day of the week (character), and the number of riders (numeric).

The initial tidyverse pipeline will conduct the following tasks, in order: 

1. The tidyverse package `readr` will be used to read the data and convert them into a tibble. To do this, the `read_delim()` function can determine the type of data by reading and initial number of rows. Alternatively, if the column names and types known, a column specification can be created in R and passed to `read_delim()`. 

1. The data are filtered to eliminate a few columns that are not needed (such as the station ID) and the column `stationname` is changed to `station`. The function `select()` is used. When filtering, the names of the column names can be used or a `dplyr()` selector function. When selecting names, a new variable name can be declared using the argument format `new_name = old_name`.

1. The date field is converted to the R date format using the `mdy()` function from the `lubridate` package. The ridership numbers are also converted to thousands. Both of these computations are executed using the `dplyr::mutate()` function.  

1. There are a small number of days that have replicate ridership numbers at certain stations. To mitigate this issue, the maximum number of rides will be used for each station and day combination. To do this, the data will be _grouped_ by station and day then the ridership data within each of the `r num_combos` unique combinations will be _summarized_ with the maximum statistic. 

The tidyverse code for these steps is:

```{r tidyverse-import}
library(tidyverse)
library(lubridate)

url <- "chi.csv"

# Pre-define the data types for the different columns that
# are imported from the URL. The `col_()` functions tell `read_delim()` what
# type of data to expect.
train_cols <- 
  cols(
    station_id  = col_double(),
    stationname = col_character(),
    date        = col_character(),
    daytype     = col_character(),
    rides       = col_double()
  )

all_stations <- 
  # Step 1: Read in the data.
  read_delim(url, delim = ",", col_types = train_cols) %>% 
  # Step 2: filter columns and rename stationname
  dplyr::select(station = stationname, date, rides) %>% 
  # Step 3: Convert the character date field to a date encoding.
  # Also, put the data in units of 1K rides
  mutate(date = mdy(date), rides = rides/1000) %>% 
  # Step 4: Summarize the multiple records using the maximum.
  group_by(date, station) %>% 
  summarize(rides = max(rides)) %>% 
  ungroup()
```

This pipeline of operations illustrates why the tidyverse is popular. A series of data manipulations are used that have simple and easy to understand user-interfaces that are bundled together in a streamlined and readable way. The focus is on how the user interacts with the software. This approach enables more people to learn R and achieve their analysis goals.  

