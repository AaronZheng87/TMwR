```{r spending-setup, include = FALSE}
knitr::opts_chunk$set(fig.path = "figures/")
library(tidymodels)
library(AmesHousing)
```

# Spending our data {#data-spending}

There are a variety of steps involved with creating a useful model. For example: parameter estimation, model selection and tuning, performance assessment, calibration, and so on. At the start of a new project, there is usually an initial pool of data available for these tasks. How should the data be applied to these tasks? The idea of _data spending_ is an important first consideration when modeling, especially as it related to empirical validation. 

When there are copious amounts of data, a smart strategy would be to allocate specific amounts of data for various tasks (as opposed to adding the largest possible amount to the model fit). With a new modeling project, there may be many questions that must be answered with limited prior knowledge. For example, if a large number of predictors are available and an excessive amount of data is available, one strategy would be to spend a specific amount on data analysis that is focused on determining which predictors are informative. As data are reused for multiple tasks, the higher the risk of adding bias and other methodological errors.

If the initial pool of data is not huge, there will be some overlap of usage and a solid methodology for data spending is important. This chapter demonstrates the basics of _splitting_ our initial pool of samples for different purposes. 

## Common methods for splitting data {#splitting-methods}

The primary approach when using empirical validation would be to split the existing pool of data into two distinct sets. Some data are used to develop and optimize the model. This _training set_ is usually the majority of the data. These data are the _sandbox_ for model building where different models can be fit, feature engineering strategies are investigated, and so on. The modeling process spends the vast majority of the time using the training set as the substrate that is used to advance the model.  

The other portion of the data are placed into the _test set_. This is held in reserve until one or two models are chosen as the methods that are mostly likely to succeed. The test set is then used as the final arbiter to determine the efficacy of the model. It is critical to only look at the test set once; otherwise it becomes part of the modeling process. 

How should we conduct this split of the data? This depends on the context. Suppose 80% of the data were to be allocated to the training set and the remaining 20% would be used for testing.  The most common method is to use simple random sampling. The `rsample` package has tools for making data splits and the function `intial_split()` was created for this purpose. It takes the data frame as an argument as well as the proportion to be **placed into training**. For example: 

```{r ames-split, message = FALSE, warning = FALSE}
library(tidymodels) # Includes rsample
library(AmesHousing)

# Remove the quality-related columns
ames <- 
  make_ames() %>% 
  dplyr::select(-matches("_Qu"))

# Set the random number stream using `set.seed()` so that the results can be 
# reproduced later. 
set.seed(833961)

# Save the split information for an 80/20 split of the data
ames_split <- initial_split(ames, prob = 0.80)
ames_split
```

The printed information denotes the amount of data in the training set ($n = `r format(nrow(training(ames_split)), big.mark = ",")`$), the amount in the test set ($n = `r format(nrow(testing(ames_split)), big.mark = ",")`$), and the size of the original pool of samples  ($n = `r format(nrow(ames), big.mark = ",")`$). 

The object `ames_split` is an `rsplit` object and only contains the partitioning information; to get the resulting data sets, two more functions are applied:

```{r ames-split-df}
ames_train <- training(ames_split)
ames_test  <-  testing(ames_split)

dim(ames_train)
```

These objects are data frames with the same columns as the original data but only contain the appropriate rows. 

```{r ames-sale-price, echo = FALSE, fig.cap = "The distribution of sale price for the Ames housing data. The vertical lines indicate the quartiles of the data."}
sale_dens <- 
  density(ames$Sale_Price, n = 2^10) %>% 
  tidy() 
quartiles <- quantile(ames$Sale_Price, probs = c(1:3)/4)
quartiles <- tibble(prob = (1:3/4), value = unname(quartiles))
quartiles$y <- approx(sale_dens$x, sale_dens$y, xout = quartiles$value)$y

quart_plot <-
  ggplot(ames, aes(x = Sale_Price)) +
  geom_line(stat = "density") +
  geom_segment(data = quartiles,
               aes(x = value, xend = value, y = 0, yend = y),
               lty = 2) +
  xlab("Sale Price (USD)")
quart_plot
```

Simple random sampling is appropriate in many cases but there are some exceptions. In classification situations where there is a severe _class imbalance_, one class may have a low frequency of occurrence. Using a simple random sample may haphazardly allocate these infrequent samples disproportionately into the training or test set. To avoid this, _stratified sampling_ can be used. For classification problems, the training/test split is conducted separately within each class and then these sub-samples are combined into the overall training and test set. For regression problems, the outcome data can be artificially binned into _quartiles_ and stratified sampling is conducted four separate times. This is an effective method for keeping the distributions of the outcomes similar between the training and test set. 

The distribution of the sale price outcome is shown in Figure \@ref(fig:ames-sale-price). As previously discussed, the sale price distribution is right-skewed. This indicates that there tends to be proportionally more expensive houses than inexpensive houses as they fall about the center of the distribution. The worry here would be that the more expensive houses are not represented in the training set and this would add the risk that the model would be ineffective at predicting such properties.  Figure \@ref(fig:ames-sale-price) contains dotted vertical lines that delineate the four quartiles for these data. A stratified random sample would conduct the 80/20 split within each of these data subsets and then pool the results together. In `rsample`, this is achieved using the `strata` argument: 

```{r ames-strata-split}
set.seed(833961)
ames_split <- initial_split(ames, prob = 0.80, strata = Sale_Price)
ames_train <- training(ames_split)
ames_test  <-  testing(ames_split)

dim(ames_train)
```

There is very little down-side to using stratified sampling. 

Are there situations when random sampling is not the best choice? One case would be when the data have a significant _time_ component, such as time series data. Here, it is more common to use the most recent data as the test set. `rsample` contains a function called `initial_time_split()` that is very similar to `initial_split()`. Instead of using random sampling, the `prop` argument denotes what proportion of the first part of the data should be used as the training set; the function assumes that the data have been pre-sorted in an appropriate order. 

## What proportion is should be used? 

The amount of data that would be allocated when splitting the data is highly dependent on the context of the problem at hand. Too much data in the training set lowers the quality of the performance estimates. Conversely, too much data in the test set handicaps the model's ability to find appropriate parameter estimates. There is a school of thought in Statistics that eschews test sets in general because all of the data should be used for parameter estimation. While there is merit to this argument, it is good scientific practice to have an unbiased set of the as the final arbiter of model quality. A test set should be avoided only when the data are pathologically small.

## What about a validation set? 

Previously, when describing the goals of data splitting, the test set was singled out as the data that should be used to conduct a proper evaluation of model performance on the models that are viewed as best. The begs the question of "How can we tell what is best if we don't measure performance until the test set?" 

It is common to hear about _validation sets_, especially in the neural network and deep learning literature. The validation set was originally defined in the early days of neural networks when there was a realization that when performance was measured by re-predicting the training set samples, the results were significantly over-optimistic. This led to models that overfit, meaning that they performed very well on the training set but poorly on the test set^[This is discussed in much greater detail in Chapter \@ref(over-fitting).]. To combat this issue, a small validation set of data were held back and these would be used to measure performance as the network was trained. Once the validation set error rate began to rise, the training would be halted. In other words, the validation set was a means to get a rough sense of how well the model performed prior to the test set. 

It is largely semantics as to whether validation sets are a subset of the training set or a third allocation in the initial split of the data. Here, validation sets are discussed more in Chapter \@ref(resampling) as a special case of _resampling_ methods that are used on the training set.

## Other considerations 

One pattern that will be seem throughout this book is the related to which data are exposed to the model at any given time. As previously mentioned, it is critical to quarantine the test set data from model building activities. The problem of _information leakage_ occurs when data outside of the training set are used in the modeling process. For example, in a machine learning competition, the test set data might be provided without the true outcome values so that the model can be scored and ranked. One potential method for improving the score might be to fit the model using the training set points that are most similar to the test set values. While the test set isn't directly used to fit the model, it still has a heavy influence. In general, this technique is highly problematic since it reduces the _generalization error_ of the model to optimize performance on a specific data set. There are more subtle ways that the test set data can be utilized during training. Keeping the training data in a separate data frame from the test set is a small check to make sure that information leakage does not occur by accident. 

Later chapters contain discussions where techniques are used to sub-sample the training set when trying to mitigate specific issues (e.g., class imbalances). This is a valid and common technique that deliberately results in the training set data diverging from the more general population from which the data were drawn. It is critical that the test set continue to mirror what the model would encounter _in the wild_. In other words, the test set should always resemble the kind of data that will be given to the model. 
 
Finally, at the beginning of this chapter, there was a warning about using the same data for different tasks. Chapter \@ref(resampling) will discuss data-driven methodologies that can be used to apply a solid methodology for data usage that will reduce the risks related to bias, overfitting, and other issues. Many of these methods mirror the data-splitting tools shown above.  
