# A review of base R modeling syntax {#base-r}

```{r base-r-setup, include = FALSE}
knitr::opts_chunk$set(fig.path = "figures/")
library(tidyverse)
library(lubridate)
```

```{r base-r-cricket-data, include = FALSE}
crickets <- 
  tibble::tribble(
           ~species,      ~temp,  ~rate,
 "O. exclamationis",       20.8,   67.9,
 "O. exclamationis",       20.8,   65.1,
 "O. exclamationis",       24.0,   77.3,
 "O. exclamationis",       24.0,   78.7,
 "O. exclamationis",       24.0,   79.4,
 "O. exclamationis",       24.0,   80.4,
 "O. exclamationis",       26.2,   85.8,
 "O. exclamationis",       26.2,   86.6,
 "O. exclamationis",       26.2,   87.5,
 "O. exclamationis",       26.2,   89.1,
 "O. exclamationis",       28.4,   98.6,
 "O. exclamationis",       29.0,  100.8,
 "O. exclamationis",       30.4,   99.3,
 "O. exclamationis",       30.4,  101.7,
 "O. niveus",              17.2,   44.3,
 "O. niveus",              18.3,   47.2,
 "O. niveus",              18.3,   47.6,
 "O. niveus",              18.3,   49.6,
 "O. niveus",              18.9,   50.3,
 "O. niveus",              18.9,   51.8,
 "O. niveus",              20.4,   60.0,
 "O. niveus",              21.0,   58.5,
 "O. niveus",              21.0,   58.9,
 "O. niveus",              22.1,   60.7,
 "O. niveus",              23.5,   69.8,
 "O. niveus",              24.2,   70.9,
 "O. niveus",              25.9,   76.2,
 "O. niveus",              26.5,   76.1,
 "O. niveus",              26.5,   77.0,
 "O. niveus",              26.5,   77.7,
 "O. niveus",              28.6,   84.7
  ) %>% 
  mutate(species = factor(species))
```

This book is about software, specifically R syntax for creating models. Before describing how tidy principles can be used in data analysis, it makes sense to show how models are created and utilized using traditional base R code. This section is a brief illustration of the those conventions. It is not exhaustive but provides readers uninitiated to R ideas about the basic motifs that are commonly used. 

The S language, on which R is based, has had a rich data analysis environment since the publication of @WhiteBook (commonly known as The White Book). This version of S introduced standard infrastructure components, such as symbolic model formulae, model matrices, data frames, as well as the standard object-oriented programming methods for data analysis. These user-interfaces have not substantively changes since then.  

## An example

To demonstrate the fundamentals, experimental data from @mcdonald2009 (by way of @mangiafico2015) are used. These data relate the ambient temperature to the rate of cricket chirps per minute. Data were collected for two species: _O. exclamationis_ and _O. niveus_. The data are contained in a data frame called `crickets` that contains a total of `r nrow(crickets)` data points. These data are shown via a `ggplot` graph. 

```{r base-r-cricket-plot, out.width = '70%', fig.width=6, fig.height=4, warning = FALSE}
names(crickets)

# Plot the temperature on the x-axis, the chirp rate on the y-axis. The plot
# elements will be colored differently for each species:
ggplot(crickets, aes(x = temp, y = rate, col = species)) + 
  # Plot points for each data point and color by species
  geom_point() + 
  # Show a simple linear model fit created separately for each species:
  geom_smooth(method = lm, se = FALSE) + 
  labs(x = "Temperature (C)", y = "Chirp Rate (per minute)")
```
 
The data show fairly linear trends for each species. For a given temperature, _O. exclamationis_ appears to have more chirps than the other species. For an inferential model, the researchers might have specified the following null hypotheses prior to seeing the data:

 * Temperature has no affect on the chirp rate.

 * There are no differences between the species in terms of chirp rate. 

There may be some scientific rationale for being able to predict the chirp rate but the focus here will be on inference.

To fit an ordinary linear model, the `lm()` function is commonly used. The important arguments to this function are a model formula and a data frame that contains the data The formula is _symbolic_. For example, the simple formula:

```r
rate ~ temp
```
states that the chirp rate is the outcome (since it is on the left-hand side of the tilde `~`) and that the temperature values are the predictor^[Most model functions implicitly add an intercept column.]. Suppose the data contained the time of day in which the measurements were obtained in a column called `time`. The formula

```r
rate ~ temp + time
```

would not add the time and temperature values together. This formula would symbolically represent that temperature and time should be added as a separate _main effects_ to the model. Main effects are model terms that contain a single predictor variable. 

There are no time measurements in these data but the species can be added to the model in the same way: 

```r
rate ~ temp + species
```

Species is not a quantitative variable; in the data frame, it is represented as a factor column with levels `"O. exclamationis"` and `"O. niveus"`. The vast majority of model functions cannot operate on non-numeric data. For species, the model needs to _encode_ the species data into a numeric format. The most common approach is to use indicator variables (also known as "dummy variables") in place of the original qualitative values. In this instance, since species has two possible values, the model formula will automatically encode this column as numeric by adding a new column that has a value of zero when the species is `"O. exclamationis"` and a value of one when the data correspond to `"O. niveus"`. The underlying formula machinery will automatically convert these values for the data set used to create the model as well as for any new data points (for example, when the model is used for prediction). 

Suppose there were five species. The model formula would automatically add _four_ additional binary columns that are binary indicators for four of the species. The _reference level_ of the factor (i.e., the first level) is always left out of the predictor set. The idea is that, if you know the values of the four indicator variables, the value of the species can be determined. 

The model formula shown above creates a model where there are different y-intercepts for each species. It is a reasonable supposition that the slopes of the regression lines could be different for each species. To accommodate this structure, an _interaction_ term can be added to the model. This can be specified in a few different ways, the most basic uses the colon:

```r
rate ~ temp + species + temp:species

# A shortcut can be used to expand all interactions containing
# interactions with two variables:
rate ~ (temp + species)^2
```

In addition to the convenience of automatically creating indicator variables, the formula offers a few other niceties: 

* _In-line_ functions can be used in the formula. For example, if the natural log of the temperate were used, the formula `rate ~ log(temp)` could be used. Since the formula is symbolic by default, literal math can also be done to the predictors using the identity function `I()`. For example, to use Fahrenheit units, the formula could be `rate ~ I( (temp * 9/5) + 32 )` to make the conversion.

* R has many functions that are useful inside of formulas. For example, `poly(x, 3)` would create linear, quadratic, and cubic terms for `x` to the model as main effects. Also, the `splines` package has several functions to create nonlinear spline terms in the formula. 

* For data sets where there are many predictors, the period shortcut is available. The period represents main effects for all of the columns that are not on the left-hand side of the tilde. For example, using `~ (.)^3` would create main effects as well as all two- and three-variable interactions to the model. 

For the initial data analysis, the two-factor interaction model is used. In this book, the suffix `_fit` is used for R objects that are fitted models. 

```{r base-r-cricket-fit}
interaction_fit <-  lm(rate ~ (temp + species)^2, data = crickets) 

# To print a short summary of the model:
interaction_fit
```

This output is a little hard to read. For the species indicator variables, R mashes the variable name (`species`) together with the factor level (`O. niveus`) with no delimiter. 

Before going into any inferential results for this model, the fit should be assessed using diagnostic plots. The `plot()` method for `lm` objects can be used. It produces a set of four plots for the object, each showing different aspects of the fit. Two plots are shown here:

```{r interaction-plots, out.width = '100%', fig.width=8, fig.height=4.5, warning = FALSE}
# Place two plots next to one another:
par(mfrow = c(1, 2))

# Show residuals vs predicted values:
plot(interaction_fit, which = 1)

# A normal quantile plot on the residuals:
plot(interaction_fit, which = 2)
```

These appear reasonable enough to conduct inferential analysis. 

From a technical standpoint, R is _lazy_. Model fitting functions typically compute the minimum possible quantities. For example, there may be interest in the coefficient table for each model term. This is not automatically computed but is instead computed via the `summary()` method. 

Our second order of business is to assess if the inclusion of the interaction term is necessary. The most appropriate approach for this model is to re-compute the model without the interaction term and use the `anova()` method. 

```{r base-r-cricket-anova}
# Fit a reduced model:
main_effect_fit <-  lm(rate ~ temp + species, data = crickets) 

# Compare the two:
anova(main_effect_fit, interaction_fit)
```

The results of the statistical test generates a p-value of `r format.pval(anova(interaction_fit, main_effect_fit)[2,6])`. This value implies that there is a lack of evidence for the alternative hypothesis that the the interaction term is needed by the model. For this reason, further analysis will be conducted on the model without the interaction. 

Residual plots should be re-assessed to make sure that our theoretical assumptions are valid enough to trust the p-values produced by the model (not shown but spoiler alert: they are). 

The `summary()` method is used to inspect the coefficients, standard errors, and p-values of each model term: 
```{r base-r-main-coef}
summary(main_effect_fit)
```

From these values, the chirp rate for each species increases by `r round(coef(main_effect_fit)[2], 2)` chirps as the temperature increases by a single degree. This term shows strong statistical significance as evidenced by the p-value.  The species term has a value of `r round(coef(main_effect_fit)[3], 2)`. This indicates that, across all temperature values, _O. niveus_ is a  chirp rate that is about `r floor(abs(coef(main_effect_fit)[3]))` fewer chirps per minute that _O. exclamationis_. Similar to the temperature term, the species effect is associated with a very small p-value.  

The only issue in this analysis is the intercept value. It indicates that at 0 C, there are negative chirps per minute for both species. While this is unreasonable, the data only go as low as `r min(crickets$temp)` C and interpreting the model at 0 C would be an _extrapolation_. This would be a bad idea. That said, the model fit is good within the _applicable range_ of the temperature values and the conclusions should be limited to the observed temperature range. 

If there were a need to estimate the chirp rate at a temperature that was not observed in the experiment, the `predict()` method would be used. It takes the model object and a data frame of new values for prediction. For example, the model estimates the chirp rate for _O. exclamationis_ for temperatures between 15 C and 20 C can be computed via:

```{r base-r-cricket-pred}
new_values <- data.frame(species = "O. exclamationis", temp = 15:20)
predict(main_effect_fit, new_values)
```

```{block, type = "rmdnote"}
Note that the non-numeric value of `species` is given to the predict method (as opposed to the binary indicator variable).  
```

While this analysis has obviously not been an exhaustive demonstration of R's modeling capabilities, it does highlight some of the major features: 

 * The language has an expressive syntax for specifying model terms for simple and fairly complex models.

 * For formula method has many conveniences for modeling that are also applied to new data when predictions are generated. 

 * There are numerous helper functions (e.g., `anova()`, `summary()` and `predict()`) that are used to conduct specific calculations after the fitted model is created. 

Finally, as previously mentioned, this framework was devised in 1992. Most of the ideas and methods above were developed in that period and have remained remarkably relavant to this day. It highlights that the S language and, by extension R, has been designed for data analysis since its inception.  


## Why tidiness is important for modeling

One of the strengths of R is that it encourages developers to create a user-interface that fits their needs.  As an example, here are three common methods for creating a scatter plot of two numeric variables residing in a data frame called `plot_data`:

```{r base-r-three-plots, eval = FALSE}
plot(plot_data$x, plot_data$y)

library(lattice)
xyplot(y ~ x, data = plot_data)

library(ggplot2)
ggplot(plot_data, aes(x = y, y = y)) + geom_point()
```

In this case, separate groups of developers devised distinct interfaces for the same task. Each has advantages and disadvantages. 

In comparison, the _Python Developer's Guide_ espouses the notion that, when approaching a problem:

> "There should be one-- and preferably only one --obvious way to do it."

The advantage of R's diversity of interfaces is that it it can evolve over time and fit different types of needs for different users. 

Unfortunately, some of the syntactical diversity is due to a focus on the developer's needs instead of the needs of the end-user. For example, one issue with some existing methods in base R is that the manner in which some data are stored may not be the most useful. Previously, the results of linear model were saved: 

```{r base-r-show-in-model}
main_effect_fit
```

The `summary()` method was used to print the results of the model fit, including a table with parameter values, their uncertainty estimates, and p-values. These particular results can also be saved:

```{r base-r-lm-param}
model_res <- summary(main_effect_fit)
# The model coefficient table is accessible via the `coef`
# method.
param_est <- coef(model_res)
class(param_est)
param_est
```

There are a few things to notice about this result. First, the object is a numeric matrix. This data structure was mostly likely chosen since all of the calculated results are numeric and a matrix object is stored more efficiently than a data frame. This choice was probably made in the late 1970's when the level of computational efficiency was critical. Second, the non-numeric data (the labels for the coefficients) are contained in the row names. Keeping the parameter labels as row names is very consistent with the conventions in the original S language. 

A reasonable course of action would be to create a visualization of the parameters values (perhaps using one of the plotting methods shown above). To do this, it would be sensible to convert the parameter matrix to a data frame. In doing so, a new column could be created with the variable names so that they can be used in the plot. However, note that several of the matrix column names would not be valid R object names (e.g. `"Pr(>|t|)"`.  Another complication is the consistency of the column names. For `lm` objects, the column for the test statistic is `"Pr(>|t|)"`. However, for other models, a different test might be used and, as a result, the column name is different (e.g., `"Pr(>|z|)"`) and the type of test is _encoded in the column name_.  
 
While these additional data formatting steps are not problematic they are a bit of an inconvenience, especially since they might be different for different types of models. The matrix is not a highly reusable data structure mostly because it must constrains the data to be of a single type (e.g. numeric). Additionally, keeping some data in the dimension names is also problematic since those data must be extracted to be of general use. For these reasons, the tidyverse places a large degree of importance on data frames and _tibbles_. Tibbles are data frames with a few extra features and, while they can use them, row names are eschewed. 

As a solution, the `broom` package has methods to convert many types of objects to a tidy structure. For example, using the `tidy()` method on the linear model produces:

```{r base-r-load-tm, include = FALSE}
library(tidymodels)
```

```{r base-r-tidy-lm}
library(tidymodels)  # includes the broom package
tidy(main_effect_fit)
```
 
The column names are standardized across models and do not contain any additional data (such as the type of statistical test). The data previously contained in the row names are now in a column called `terms` and so on. One additional principle in the tidymodels ecosystem is that a functions return values should be **predictable, consistent, and unsurprising**. 
 
As another example of _unpredictability_, another convention in base R is related to missing data. The general rule is that missing data propagate more missing data; the average of a set of values with a missing data point is itself missing and so on. When models make predictions, the vast majority require all of the predictors to have complete values. There are several options based in to R at this point in the form of `na.action`.  This sets the policy for how a function should behave if there are missing values. The two most common policies are `na.fail` and `na.omit`. For former produces an error of missing data are involved while the latter removes the missing data prior to the calculations by case-wise deletion. From our previous example:

```{r base-r-lm-missing, error = TRUE}
# Add a missing value to the prediction set
new_values$temp[1] <- NA

# The predict method for `lm` defaults to `na.pass`:
predict(main_effect_fit, new_values)

# Alternatively 
predict(main_effect_fit, new_values, na.action = na.fail)

predict(main_effect_fit, new_values, na.action = na.omit)
```

From a user's point of view, `na.omit()` can be problematic. In our example, `new_values` has `r nrow(new_values)` rows but only `r nrow(new_values) - 1` would be returned. To compensate for this, the user would have to determine which row had the missing value and interleave a missing values in the appropriate place if the predictions were merged into `new_values`^[A base R policy called `na.exclude()` does exactly this.]. While it is rare that a prediction function uses `na.omit()` as its missing data policy, this does occur. Users who have determined this as the cause of an error in their code find it _quite memorable_. 

Finally, one other potential stumbling block can be inconsistencies between packages. Suppose a modeling project had an outcome with two classes. There are a variety of statistical and machine learning models that can be used. In order to produce class probability estimate for each sample, it is common for a model function to have a corresponding `predict()`method. However, there is significant heterogeneity in the argument values used by those methods to make class probability predictions. A sampling of these argument values for different models is: 

| Function     | Package      | Code                                       |
| :----------- | :----------- | :----------------------------------------- |
| `lda`        | `MASS`       | `predict(object)`                             |
| `glm`        | `stats`      | `predict(object, type = "response")`          |
| `gbm`        | `gbm`        | `predict(object, type = "response", n.trees)` |
| `mda`        | `mda`        | `predict(object, type = "posterior")`         |
| `rpart`      | `rpart`      | `predict(object, type = "prob")`              |
| various      | `RWeka`      | `predict(object, type = "probability")`       |
| `logitboost` | `LogitBoost` | `predict(object, type = "raw", nIter)`        |
| `pamr.train` | `pamr`       | `pamr.predict(object, type = "posterior")`    |

Note that the last example has a custom _function_ to make predictions instead of using the model common `predict()` interface.  

There are a few R packages that provide a unified interface to harmonize these modeling APIs, such as `caret` and `mlr`. tidymodels takes a similar approach to unification of the function interface as well as enforcing consistency in the function names and return values (e.g., `broom::tidy()`).  


## Some additional tidy principals for modeling. 

To resolve the usage issues described in the last section, the tidymodels packages have a few additional design goals that complement those of the tidyverse. However, a considerable amount of the tidymodels goals fall under the existing rubric of _Design for Humans_ but for modeling code. Some examples: 

* Make argument and function names less _jargony_. For example, many statistical regularization methods use the greek letter lambda ($\lambda$) to represent the amount of penalization that is used during parameter estimation. While very specific, the only users who know what this means are those who have read those parts of the technical literature. When naming things, we prefer more self-documenting (and obvious) choices be used. For example, "penalty" is probably a better name than "lambda". 

* R has excellent capabilities for _object oriented programming_ and this should be used in lieu of creating new function names (such as `predict_samples()`). 

* _Sensible defaults_ are very important. Whenever possible, functions should have no default for arguments when only when a default is impossible or if you want to force the user to make a choice. The latter should be used sparingly.

* Similarly, argument values whose default _can_ be derived from the data should be. For example, for `glm()` the `family` argument could check the type of data in the outcome and, if no `family` was given, a default could be determined internally.

* Functions should take the **data structures that users have** as opposed to the data structure required by the underlying computational code. For example, a model function's _only_ interface should not be constrained to matrices. Frequently, users will have non-numeric predictors such as factors. 

Many of these ideas are described in the tidymodels guidelines for model implementation^[`https://tidymodels.github.io/model-implementation-principles`]

A few more principles are discussed below. 

**Be predictable, consistent, and unsurprising**

As seen in the previous section, consistency can be lacking between and within R packages. The tidymodels packages work to 

 * Follow a consistent syntax in new functions. 

 * Provide a consistent interface for existing functions. 

 * Enforce consistency of _return values_ of functions. 

The last point is a good idea for functional programming but also to reduce frustrations for users. Previously, it was mentioned that the `summary()` method for some functions has column names that changed with the type of model (e.g. `"Pr(>|t|)"`) and the `broom` package was used to harmonize the results with the `tidy()` methods. 

There are other, more difficult examples, especially as they relate to model predictions. For example, the `ranger` package is an excellent tool for computing random forest models. However, instead of returning a data frame or vector as input, a specialized object is returned that have multiple values embedded (including the predicted values). This is just one more step for the data analyst to work around in their scripts. As another example, the `glmnet` package can return at least four different output types for predictions, depending on the model and characteristics of the data: 

| Type of Prediction       | Returns a:                      |
|--------------------------|---------------------------------|
| numeric                  | numeric matrix                  |
| class                    | _character_ matrix              |
| probability (2 classes)  | numeric matrix (2nd level only) |
| probability (3+ classes) | 3D numeric array (all levels)   |

Additionally, the column names of the results contain coded values that map to vector in the `glmnet` model object. This excellent statistical method can be discouraging to use in practice because of all of the special cases one might encounter that require additional code to be useful. 

The tidymodels packages always return consistent predicted values that follow several rules:

 * The results are always a tibble. 

 * The number of rows in the tibble always match the number of samples being predicted, regardless of missing values. 

 * The names and formats of the columns are predictable.

Finally, to help new R packages, the next section describes a tidymodels package called `hardhat` that enables package authors implement consistent interfaces and return values. 

**Encourage empirical validation and good methodology**

Enable a wider variety of methodologies

Protect users from making objectively poor choices. Examples:

- *Information leakage* of training set data into evaluation sets.
- Analyzing integers as categories
- Down-sampling the test set

These examples relate directly to the _pit of success_ mentioned earlier.  

One aspect of machine learning models that is applicable to the majority model types is the use of a separate set of data to verify how well the model performs. _Data splitting_ usually reserves a set of the available data that is used after the model building activities are finished. These data serve as an unbiased method for evaluating the capabilities of the model. There is an emphasis on such _empirical validation_ in tidymodels is a more robust approach to establish the performance characteristics of the model.

Similarly, tidymodels strongly leverage resampling methods as a way to empirically characterize characteristics of models. Resampling methods, discussed in Chapter \@ref(resampling), is a type of simulation system that can re-evaluate the model under somewhat different scenarios. This enables the model to be characterized in the context of the observed data as opposed to an unobserved theoretical distribution. For example, in linear regression, we commonly assume normality of the model residuals and base confidence intervals and other quantities on this assumption. Using resampling, similar statistical quantities can be computed but using the _empirical distribution_ of the data. While  theoretical and empirical distribution can results in very similar results, it helps to have an additional verification of the results based on the _data at hand_. 

## Combining base R models and the tidyverse

Traditional R modeling functions can be used in conjunction with the tidyverse, especially with the `dplyr`, `purrr`, and `tidyr` packages. For example, if there was interest in fitting separate models for each species, the data can first be broken out by this column using `dplyr::group_by()` and `tidy::nest()`: 

```{r base-r-by-species-split}
split_by_species <- 
  crickets %>% 
  group_by(species) %>% 
  nest()
split_by_species
```

The `data` column contains the `rate` and `temp` columns from `crickets` in a _list column_. From this, the `purrr::map()` function can create individual models for each species:

```{r base-r-species-models}
model_by_species <- 
  split_by_species %>% 
  mutate(model = map(data, ~ lm(rate ~ temp, data = .x)))
model_by_species
```

If there is interest in collecting the coefficients for each of these models, it is helpful to use `broom::tidy()` to convert them to a consistent data frame format and then these can be unested:

```{r base-r-species-coefs}
model_by_species %>% 
  mutate(coef = map(model, tidy)) %>% 
  select(species, coef) %>% 
  unnest(cols = c(coef))
```

As previously mentioned, list columns can be very powerful to use. These provide containers for any type of R objects that can be coupled with the helpful data frame structure. 
 