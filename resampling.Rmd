```{r resampling-setup, include = FALSE}
knitr::opts_chunk$set(fig.path = "figures/")
library(tidymodels)
library(AmesHousing)
library(doMC)
library(kableExtra)

ames <- make_ames()

set.seed(833961)
ames_split <- initial_split(ames, prob = 0.80, strata = Sale_Price)
ames_train <- training(ames_split)
ames_test  <-  testing(ames_split)

registerDoMC(cores = parallel::detectCores())
```

# Resampling for evaluating performance  {#resampling}

which metric vs how estimated

advocated data splitting and test set but...


## The resubstitution approach

The Ames data will be used once again to demonstrate the concepts in this chapter. From Chapter \@ref(recipes), a useful recipe for these data was:  

```{r resampling-ames-rec}
# First make a very basic recipe that will be used by multiple models
basic_rec <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
           Latitude + Longitude, data = ames_train) %>%
  step_log(Sale_Price, base = 10)

# Add to the basic recipe to re-create the Ames recipe from Chapter TODO
ames_rec <- 
  basic_rec %>%
  step_log(Gr_Liv_Area, base = 10) %>% 
  step_other(Neighborhood, threshold = 0.01) %>% 
  step_dummy(all_nominal()) %>% 
  step_interact(~ Gr_Liv_Area:starts_with("Bldg_Type_")) %>% 
  step_ns(Latitude, Longitude, deg_free = 20)
```

Using the training and test set split developed in Chapter \@ref(splitting), an ordinary linear regression model is fit to the training set contained in the data frame `ames_train` using a workflow object: 

```{r resampling-lm}
# Fit a simple linear regression model using this recipe. 
lm_fit <- 
  workflow() %>% 
  add_recipe(ames_rec) %>% 
  add_model(linear_reg() %>% set_engine("lm")) %>% 
  fit(data = ames_train)
```

A different type of model is also fit to these data. _Random forests_ is a tree ensemble method that creates a large number of decision trees that are somewhat different. This collection of trees make up the ensemble. When making predictions, a new sample is predicted by each ensemble member and these predictions are averaged and this average is the final ensemble prediction. 

Random forest models are very powerful; these have been shown to be able to emulate the underlying data patterns very closely. While this model can be computationally intensive it is very low-maintainence. Very little pre-processing is required (as documented in Appendix \@ref(pre-proc-table)).

Using the same predictor set as the previous linear model (without the extra pre-processing steps), the random forest model is also fit to the training set (using the underlying `ranger` package): 

```{r resampling-rand-forest-spec}
rf_model <- 
  rand_forest(trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("regression")

rf_fit <- 
  workflow() %>% 
  add_recipe(basic_rec) %>% 
  add_model(rf_model) %>% 
  fit(data = ames_train)
```

There are two models to compare now. How should this be conducted? For demonstration, the training set is predicted to produce what is known as the "apparent error rate" or the  "resubstitution error rate". A function is used to make these predictions and to used some minimal formatting of the results: 

```{r resampling-eval-func}
estimate_perf <- function(model, dat) {
  # Capture the names of the objects used
  cl <- match.call()
  obj_name <- as.character(cl$model)
  data_name <- as.character(cl$dat)
  data_name <- gsub("ames_", "", data_name)
  
  # Estimate these metrics:
  reg_metrics <- metric_set(rmse, rsq)
  
  model %>% 
    predict(dat) %>% 
    bind_cols(
      dat %>% 
        select(Sale_Price) %>% 
        mutate(Sale_Price = log10(Sale_Price))
    ) %>% 
    reg_metrics(Sale_Price, .pred) %>% 
    select(-.estimator) %>% 
    mutate(object = obj_name, data = data_name)
}
```

Both RMSE and R<sup>2</sup> are computed for the data set. For both models, the resubstitution results are: 

```{r resampling-eval-train}
estimate_perf(rf_fit, ames_train)
estimate_perf(lm_fit, ames_train)
```

```{r resampling-eval-train-results, include = FALSE}
all_res <- 
  bind_rows(
    estimate_perf(lm_fit, ames_train),
    estimate_perf(rf_fit, ames_train),
    estimate_perf(lm_fit, ames_test),
    estimate_perf(rf_fit, ames_test)
  ) %>% filter(.metric == "rmse") %>% 
  select(-.metric) %>% 
  pivot_wider(id_cols = object,
              values_from = ".estimate",
              names_from = "data")

tr_ratio <- round(all_res$train[1]/all_res$train[2], 2)
```

From these results, the random forest is much more capable of predicting the sale prices with a RMSE estimate that is `r tr_ratio`-fold better than linear regression. If these were the models that were investigated, the random forest model would probably be chosen to be the final model. The next step would be to apply it to the test set to get a final verification how well the model functions:

```{r resampling-eval-test-rf, cache = TRUE}
estimate_perf(rf_fit, ames_test)
```

The test set RMSE estimate, `r all_res %>% filter(object == "rf_fit") %>% pull("test")`, is much worse than the training set value of `r all_res %>% filter(object == "rf_fit") %>% pull("train")`. Why did this happen? 

Many predictive models are very capable of representing complex trends in the data. In statistics, these are commonly referred to as _low bias models_. 

```{block, type = "rmdnote"}
In this context, _bias_ is the different between the true data pattern and the types of patterns that the model can emulate. Many black-box machine learning models are low bias. Other models (such as linear/logistic regression, discriminant analysis, and others) are not as adaptable and would be considered _high-bias_ models. See Section [1.2.5](https://bookdown.org/max/FES/important-concepts.html#model-bias-and-variance) of <span class="citation">Kuhn and Johnson (<a href="#ref-fes" role="doc-biblioref">2020</a>)</span> for a discussion.
```

For a low-bias techniques, model complexity can sometimes result in the model to nearly memorizing the training set data. As an obvious example, consider a 1-nearest neighbor model -- it will always provide perfect predictions for the training set no matter how well the model truly works on other data sets. Random forest effectively has the same characteristic; re-predicting the training set will always result in an artificially optimistic estimate of performance.  

For both fitted models, this table summarizes the RMSE estimate for both data sets: 

```{r resampling-rmse-table, echo = FALSE, results = "asis"}
all_res %>% 
  mutate(object = paste0("<tt>", object, "</tt>")) %>% 
  kable(escape = FALSE) %>% 
  kable_styling(full_width = FALSE) %>% 
  add_header_above(c(" ", "RMSE Estimates" = 2))
```

Notice that the linear regression model did not have the same issue since the terms in the model are not highly complex. This model _could_ be made to be low-bias by adding more complex model terms that are appropriate for the data^[It is possible to show that a linear model can memorize the training set. In the `ames_rec` object, change the number of spline terms for `longitude` and `latitude` to a large number (say 1000). This would produce a model fit with a very small resubstitution RMSE and a test set RMSE that is much larger.]. The main take-away from this example is that re-predicting the training set is a bad idea for most models. 

If the test set should not be used immediately and re-predicting the training set is a bad idea, what alternative is there for evaluating the models as they are being developed?  The answer is _resampling methods_, which includes techniques such as cross-validation or validation sets. 


## Resampling methods

```{r resampling-scheme, echo = FALSE, out.width = '85%'}
if (knitr:::is_html_output()) {
  file.copy("premade/resampling.svg", "_book/premade/resampling.svg")
  knitr::include_graphics("premade/resampling.svg")
} else {
  file.copy("premade/resampling.pdf", "_book/premade/resampling.pdf")
  knitr::include_graphics("premade/resampling.pdf")
}
```

The next three section provide details on several important resampling methods and their variants. 


### Cross-validation {#cv}

Cross-validation is an old resampling technique. While there are a number of variations, the most common cross-validation method is called _V_-fold cross-validation. In this case, the data are randomly partitioned into _V_ sets of roughly equal size (called the "folds"). For illustration, _V_ = 3 is shown below for a data set of thirty training set points and a completely random fold allocation. 

```{r resampling-three-cv, echo = FALSE, out.width = '50%'}
if (knitr:::is_html_output()) {
  file.copy("premade/three-CV.svg", "_book/premade/three-CV.svg")
  knitr::include_graphics("premade/three-CV.svg")
} else {
  file.copy("premade/three-CV.pdf", "_book/premade/three-CV.pdf")
  knitr::include_graphics("premade/three-CV.pdf")
}
```

The number inside the symbols are the sample numbers while the color and shape of the symbols represent their randomly assigned folds. The assignment to folds can be done via random sampling or, as was discussed on Section \@ref(splitting-methods), stratified assignment can be used. 

For 3-fold cross-validation, the three iterations of resampling are illustrated below. The first iteration removes the data for the first fold and fits the model to the remaining two folds. This model is used to predict the data from the first fold and model performance is estimated from these data. On the second iteration, the model is fit not the first and third folds and the second fold is used to compute another set of performance metrics. A similar process is used for the third fold. 

```{r resampling-three-cv-iter, echo = FALSE, out.width = '70%'}
if (knitr:::is_html_output()) {
  file.copy("premade/three-CV-iter.svg", "_book/premade/three-CV-iter.svg")
  knitr::include_graphics("premade/three-CV-iter.svg")
} else {
  file.copy("premade/three-CV-iter.pdf", "_book/premade/three-CV-iter.pdf")
  knitr::include_graphics("premade/three-CV-iter.pdf")
}
```

The result is a collection of three replicates for each of the performance statistics. The final resampling estimate of performance averages each of the _V_ replicates. 

_V_ = 3 is a good choice to illustrate cross-validation but is a poor choice in practice. Values of _V_ are most often 5 or 10; here, we generally prefer 10-fold cross-validation as a default. 

```{block, type = "rmdnote"}
What are the effects of changing _V_? Larger values result in resampling estimates with reduced bias but increased noise. Smaller bias have large bias but better noise. 10-fold is preferred since noise can be reduced by replication, as shown below, but bias cannot. See [Section **3.4**]() of @fes for a longer description. 
```

use ames as example

```{r resampling-ames-cv}
set.seed(1352)
ames_folds <- vfold_cv(ames_train, v = 10)
ames_folds

class(ames_folds)
```

split object `[2K/220]`. 

replication

Due to the central limit theorem, the summary statistics that are produced by each model tend toward a normal distribution. For example, for the RMSE statistic, the mean residual is computed using about `r floor(nrow(ames_train) * .1)` houses when 10-fold cross-validation is used. A collection of ten of these statistics can often be reasonably  approximated using a Gaussian distribution. If this collection has a theoretical standard deviation of $\sigma$, so the standard error is $\sigma/\sqrt{10}$. How would replication affect this? Using 5- and 10-fold cross-validation with $R$ replicates, this plot shows how quickly the standard error^[Even if the statistics truly follow a Gaussian distribution, these are _approximate_ standard errors. As will be discussed in the next chapter, there is a within-replicate correlation that is typical of resampled results. By ignoring this extra component of variation, the simple calculations shown in the plot are over-estimates of the reduction in noise in the standard errors.] decreases with replicates: 

```{r resampling-cv-reduction, echo = FALSE}
cv_info <- 
  tibble(replicates = rep(1:10, 2), V = 10) %>% 
  mutate(B = V * replicates, reduction = 1/B, V = format(V))

ggplot(cv_info, aes(x = replicates, y = reduction)) + 
  geom_line() + 
  geom_point() + 
  labs(
    y = expression(paste("Multiplier on ", sigma)),
    x = "Number of 10F-CV Replicates"
  ) +
  theme_bw() + 
  scale_x_continuous(breaks = 1:10)
```

Larger number of replicates tend to have less impact on the standard error. If the baseline value of $\sigma$ is impractically large, the diminishing returns on replication may still be worth the extra computational power. 

Other types of rs: loo and mc



### Validation sets {#validation}


```{r resampling-validation, echo = FALSE, out.width = '50%'}
if (knitr:::is_html_output()) {
  file.copy("premade/validation.svg", "_book/premade/validation.svg")
  knitr::include_graphics("premade/validation.svg")
} else {
  file.copy("premade/validation.pdf", "_book/premade/validation.pdf")
  knitr::include_graphics("premade/validation.pdf")
}
```


### Bootstrapping {#bootstrap}

```{r resampling-bootstraps, echo = FALSE, out.width = '90%'}
if (knitr:::is_html_output()) {
  file.copy("premade/bootstraps.svg", "_book/premade/bootstraps.svg")
  knitr::include_graphics("premade/bootstraps.svg")
} else {
  file.copy("premade/bootstraps.pdf", "_book/premade/bootstraps.pdf")
  knitr::include_graphics("premade/bootstraps.pdf")
}
```



### Rolling origin forcasting {#rolling}


```{r resampling-rolling, echo = FALSE, out.width = '75%'}
if (knitr:::is_html_output()) {
  file.copy("premade/rolling.svg", "_book/premade/rolling.svg")
  knitr::include_graphics("premade/rolling.svg")
} else {
  file.copy("premade/rolling.pdf", "_book/premade/rolling.pdf")
  knitr::include_graphics("premade/rolling.pdf")
}
```




## Estimating performance {#resampling-performance}

Any of these resampling methods can be used to effectively measure performance of a modeling process (e.g., pre-processing, model fitting, etc). They do this in an effective way by having different groups of data that are used to fit and assess a model. To reiterate the process: 

1. During resampling, the analysis set is used to pre-process the data using the training set, apply the pre-processing to the training set, and then use these process data to fit the model. 

2. The assessment set is used to estimate performance for all of the preceding estimation steps.  

This sequence is conducted for every resample. If there are _B_ resamples, there are _B_ replicates of each of the performance metrics. The final resampling estimate is the average of these _B_ statistics. If _B_ = 1, as with a validation set, the individual statistic is used to represent overall performance. 

Let's reconsider the random forest model used previously. 

```{r resampling-cv-ames}
keep_pred <- control_resamples(save_pred = TRUE)

rf_wflow <- 
  workflow() %>% 
  add_recipe(basic_rec) %>% 
  add_model(rf_model) 

set.seed(598)
rf_res <- 
  rf_wflow %>% 
  fit_resamples(resamples = ames_folds, control = keep_pred)
rf_res
```
```{r resampling-checkpoint, include = FALSE}
lm_wflow <- 
  workflow() %>% 
  add_recipe(ames_rec) %>% 
  add_model(linear_reg() %>% set_engine("lm"))
save(rf_res, lm_wflow, rf_wflow, file = "RData/resampling.RData", version = 2, compress = "xz")
```


```{r resampling-cv-stats}
collect_metrics(rf_res)
```

```{r resampling-cv-replicates}
collect_metrics(rf_res, summarize = FALSE) %>% 
  select(-.estimator) %>% 
  pivot_wider(id_cols = c(id), names_from = .metric, values_from = .estimate)
```


```{r resampling-cv-pred}
assess_res <- collect_predictions(rf_res)
assess_res
```


```{r resampling-cv-pred-plot}
assess_res %>% 
  ggplot(aes(x = Sale_Price, y = .pred)) + 
  geom_point(alpha = .15) +
  geom_abline(col = "red") + 
  coord_equal() + 
  ylab("Predicted")
```


```{r resampling-val-ames}
set.seed(4290)
val_set <- mc_cv(ames_train, times = 1, prop = 3/4)

val_res <- 
  rf_wflow %>% 
  fit_resamples(resamples = val_set)
val_res

collect_metrics(val_res)
```


```{block, type = "rmdnote"}
In these analyses, the resampling results are very close to the test set results. The two types of estimates tend to be well correlated. However, this could be from random chance. A seed value of 1352 was used to fix the random numbers before creating the resamples. Try changing this value and re-running the analyses to investigate of the resampled estimates match the test set results as well.
```

## Parallel processing {#parallel}

## Saving the resampled objects

 