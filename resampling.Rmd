```{r resampling-setup, include = FALSE}
knitr::opts_chunk$set(fig.path = "figures/")
library(tidymodels)
library(doMC)
library(kableExtra)
library(tidyr)

registerDoMC(cores = parallel::detectCores())

source("ames_snippets.R")
load("RData/lm_fit.RData")
```

# Resampling for evaluating performance  {#resampling}

Previously, Chapter \@ref(performance), described statistics related to model performance. Another aspect of performance evaluation is "_Which data are used to compute these statistics_?". Chapter \@ref(splitting) introduced the idea of data spending where the test set was recommended for obtaining an unbiased estimate of performance. 

However, we usually need to understand the effectiveness of the model _before using the test set_. In fact, one could not decide on _which_ final model to take to the test set without making model assessments. 

In this chapter, an approach called resampling is described that can fill this gap. Resampling estimates of performance can generalize to new data. The next chapter complements this one by demonstrating statistical methods that compare resampling results. 

To motivate this chapter, the next section demonstrates how naive performance estimates can fail. 

## The resubstitution approach

The Ames data is used once again to demonstrate the concepts in this chapter. Section \@ref(workflows-summary) encapsulates the state of the Ames analysis. It includes a recipe object named `ames_rec` as well as a workflow using that recipe along with a linear model (called `lm_wflow`). This workflow was fit on the training set, resulting in `lm_fit`. 

For a comparison, a different type of model is also fit. _Random forests_ is a tree ensemble method that creates a large number of decision trees from slightly different versions of the training set [@breiman2001random]. This collection of trees make up the ensemble. When predicting a new sample, each ensemble member predicts creates a separate prediction. These are averaged to create the final ensemble prediction for the new data point. 

Random forest models are very powerful; they can emulate the underlying data patterns very closely. While this model can be computationally intensive, it is very low-maintenance. Very little preprocessing is required (as documented in Appendix \@ref(pre-proc-table)).

Using the same predictor set as the linear model (without the extra preprocessing steps), a random forest is fit to the training set using via the `ranger` package. This model requires no preprocessing so a simple formula can be used:

```{r resampling-rand-forest-spec}
rf_model <- 
  rand_forest(trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("regression")

rf_wflow <- 
  workflow() %>% 
  add_formula(
    Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
      Latitude + Longitude) %>% 
  add_model(rf_model) 

rf_fit <- rf_wflow %>% fit(data = ames_train)
```

How should the two models be compared? For demonstration, the training set is predicted to produce what is known as the "apparent error rate" or the "resubstitution error rate". A function is used to create predictions and format the results: 

```{r resampling-eval-func}
estimate_perf <- function(model, dat) {
  # Capture the names of the objects used
  cl <- match.call()
  obj_name <- as.character(cl$model)
  data_name <- as.character(cl$dat)
  data_name <- gsub("ames_", "", data_name)
  
  # Estimate these metrics:
  reg_metrics <- metric_set(rmse, rsq)
  
  model %>% 
    predict(dat) %>% 
    bind_cols(dat %>% select(Sale_Price)) %>% 
    reg_metrics(Sale_Price, .pred) %>% 
    select(-.estimator) %>% 
    mutate(object = obj_name, data = data_name)
}
```

Both RMSE and R<sup>2</sup> are computed. The resubstitution statistics are: 

```{r resampling-eval-train}
estimate_perf(rf_fit, ames_train)
estimate_perf(lm_fit, ames_train)
```

```{r resampling-eval-train-results, include = FALSE}
all_res <- 
  bind_rows(
    estimate_perf(lm_fit, ames_train),
    estimate_perf(rf_fit, ames_train),
    estimate_perf(lm_fit, ames_test),
    estimate_perf(rf_fit, ames_test)
  ) %>% filter(.metric == "rmse") %>% 
  select(-.metric) %>% 
  pivot_wider(id_cols = object,
              values_from = ".estimate",
              names_from = "data")

tr_ratio <- round(all_res$train[1]/all_res$train[2], 2)
```

Based on these results, the random forest is much more capable of predicting the sale prices; the RMSE estimate is `r tr_ratio`-fold better than linear regression. If these were the two models under consideration, the random forest would probably be chosen. The next step would be to apply it to the test set to get a final verification:

```{r resampling-eval-test-rf, cache = TRUE}
estimate_perf(rf_fit, ames_test)
```

The test set RMSE estimate, `r all_res %>% filter(object == "rf_fit") %>% pull("test")`, is **much worse than the training set**  value of `r all_res %>% filter(object == "rf_fit") %>% pull("train")`. Why did this happen? 

Many predictive models are very capable of representing complex trends in the data. In statistics, these are commonly referred to as _low bias models_. 

:::rmdnote
In this context, _bias_ is the difference between the true data pattern and the types of patterns that the model can emulate. Many black-box machine learning models have low bias. Other models (such as linear/logistic regression, discriminant analysis, and others) are not as adaptable and are considered _high-bias_ models. See Section [1.2.5](https://bookdown.org/max/FES/important-concepts.html#model-bias-and-variance) of @fes for a discussion.
:::

For a low-bias models, the high degree of predictive capacity can sometimes result in the model nearly memorizing the training set data. As an obvious example, consider a 1-nearest neighbor model. It will always provide perfect predictions for the training set no matter how well it truly works for other data sets. Random forest has a similar effect; re-predicting the training set will always result in an artificially optimistic estimate of performance.  

For both models, this table summarizes the RMSE estimate for the training and test sets: 

```{r resampling-rmse-table, echo = FALSE, results = "asis"}
all_res %>% 
  mutate(object = paste0("<tt>", object, "</tt>")) %>% 
  kable(escape = FALSE) %>% 
  kable_styling(full_width = FALSE) %>% 
  add_header_above(c(" ", "RMSE Estimates" = 2))
```

Notice that the linear regression model had consistent results due to their limited complexity. This  for this model can be decreased by adding more complex model terms^[It is possible to show that a linear model can nearly memorize the training set. In the `ames_rec` object, change the number of spline terms for `longitude` and `latitude` to a large number (say 1000). This would produce a model fit with a very small resubstitution RMSE and a test set RMSE that is much larger.]. 

:::rmdnote
The main take-away from this example is that re-predicting the training set is a bad idea for most models. 
:::

If the test set should not be used immediately, and re-predicting the training set is a bad idea, what should be done?  _Resampling methods_, such as cross-validation or validation sets, are the solution.


## Resampling methods

These techniques are empirical simulation systems that emulate the process of using some data for modeling and different data for evaluation. Most resampling methods are iterative, meaning that this process is repeated multiple times. This diagram illustrates how resampling methods generally operate:

```{r resampling-scheme, echo = FALSE, out.width = '85%'}
if (knitr:::is_html_output()) {
  file.copy("premade/resampling.svg", "_book/premade/resampling.svg")
  knitr::include_graphics("premade/resampling.svg")
} else {
  file.copy("premade/resampling.pdf", "_book/premade/resampling.pdf")
  knitr::include_graphics("premade/resampling.pdf")
}
```

**Resampling is only conducted on the training set**. The test set is not involved. For each iteration of resampling, the data are partitioned into two subsamples: 

 * The model is fit with **analysis set**. 

 * The model is evaluated with the **assessment set**. 

These are somewhat analogous to training and test sets. Our language of _analysis_ and _assessment_ avoids confusion with initial split of the data. These data sets are mutually exclusive. The partitioning scheme used to create the analysis and assessment sets is usually the defining characteristic of the method.

Suppose 20 iterations of resampling are conducted. This means that 20 separate models are fit on the analysis sets and the corresponding assessment sets produce 20 sets of performance statistics. The final estimate of performance for a model is the average of the 20 replicates of the statistics. This average has very good generalization properties and is far better than the resubstituion estimates. 

The next section defines several commonly used methods and discusses their pros and cons. 

### Cross-validation {#cv}

Cross-validation is a well established resampling method.While there are a number of variations, the most common cross-validation method is _V_-fold cross-validation. The data are randomly partitioned into _V_ sets of roughly equal size (called the "folds"). For illustration, _V_ = 3 is shown below for a data set of thirty training set points with random fold allocations. 

```{r resampling-three-cv, echo = FALSE, out.width = '50%'}
if (knitr:::is_html_output()) {
  file.copy("premade/three-CV.svg", "_book/premade/three-CV.svg")
  knitr::include_graphics("premade/three-CV.svg")
} else {
  file.copy("premade/three-CV.pdf", "_book/premade/three-CV.pdf")
  knitr::include_graphics("premade/three-CV.pdf")
}
```

The number inside the symbols is the sample number. The color of the symbols represent their randomly assigned folds. Stratified sampling is also an option for assigning folds (previously discussed in Section \@ref(splitting-methods)). 

For 3-fold cross-validation, the three iterations of resampling are illustrated below. For each iteration, one fold is held out for assessment statistics and the remaining folds are substrate for the model. This process continues for each fold so that three models produce three sets of performance statistics. 

```{r resampling-three-cv-iter, echo = FALSE, out.width = '70%'}
if (knitr:::is_html_output()) {
  file.copy("premade/three-CV-iter.svg", "_book/premade/three-CV-iter.svg")
  knitr::include_graphics("premade/three-CV-iter.svg")
} else {
  file.copy("premade/three-CV-iter.pdf", "_book/premade/three-CV-iter.pdf")
  knitr::include_graphics("premade/three-CV-iter.pdf")
}
```

When _V_ = 3, the analysis sets are 2/3 of the training set and each assessment set is a distinct 1/3. The final resampling estimate of performance averages each of the _V_ replicates. 

_V_ = 3 is a good choice to illustrate cross-validation but is a poor choice in practice. Values of _V_ are most often 5 or 10; here, we generally prefer 10-fold cross-validation as a default. 

:::rmdnote
What are the effects of changing _V_? Larger values result in resampling estimates with small bias but substantial noise. Smaller values of _V_ have large bias but low noise. We prefer 10-fold since noise is reduced by replication, as shown below, but bias is not. See [Section **3.4**](https://bookdown.org/max/FES/resampling.html) of @fes for a longer description. 
:::

The primary input is the training set data frame as well as the number of folds (defaulting to 10): 

```{r resampling-ames-cv}
set.seed(55)
ames_folds <- vfold_cv(ames_train, v = 10)
ames_folds
```

The column named `splits` contains the information on how to split the data (similar to the object used to create the initial training/test partition). While each row of `splits` has an embedded copy of the entire training set, R is smart enough not to make copies of the data in memory. The print method inside of the tibble shows the frequency of each: `[2K/220]` indicates that roughly two thousand samples are in the analysis set and 220 are in that particular assessment set. 

To manually retrieve the partitioned data, the `analysis()` and `assessment()` functions return the corresponding data frames: 

```{r resampling-analysis}
# For the first fold:
ames_folds$splits[[1]] %>% analysis() %>% dim()
```

tidymodels packages, such as `tune`, contain high-level user interfaces so that these functions are not generally needed for day-to-day work. The next section demonstrates a function to fit a model over these resamples. 

These `rsample` objects also always contain a character column called `id` that labels the partition. Some resampling methods require multiple `id` fields.  


### Repeated cross-validation {-}

There are a variety of cross-validation variations. The most important is _repeated_ _V_-fold cross-validation. Depending on the size or other characteristics of the data, the resampling estimate produced by _V_-fold cross-validation may be excessively noisy^[For more details, see [Section 3.4.6](https://bookdown.org/max/FES/resampling.html#resample-var-bias) of @fes.]. As with many statistical problems, one way to reduce noise is to gather more data. For cross-validation, this means averaging more than _V_ statistics. 

To create _R_ repeats of _V_-fold cross-validation, the same fold generation process is redone _R_ times to generate _R_ collections of _V_ partitions. Now, instead of averaging _V_ statistics, $V \times R$ statistics produce the final resampling estimate.

Due to the Central Limit Theorem, the summary statistics from each model tend toward a normal distribution. Consider the Ames data. On average, 10-fold cross-validation uses  assessment sets that contain roughly `r floor(nrow(ames_train) * .1)` properties. 

If the RMSE is the statistic of choice, we'll denote that estimate's standard deviation as $\sigma$. With simple 10-fold cross-validation, the standard error of the mean RMSE is $\sigma/\sqrt{10}$. If this is too noisy, repeats reduce the standard error to $\sigma/\sqrt{10R}$. For 10-fold cross-validation with $R$ replicates, the plot below shows how quickly the standard error^[These are _approximate_ standard errors. As will be discussed in the next chapter, there is a within-replicate correlation that is typical of resampled results. By ignoring this extra component of variation, the simple calculations shown in this plot are overestimates of the reduction in noise in the standard errors.] decreases with replicates: 

```{r resampling-cv-reduction, echo = FALSE, fig.height= 4}
cv_info <- 
  tibble(replicates = rep(1:10, 2), V = 10) %>% 
  mutate(B = V * replicates, reduction = 1/B, V = format(V))

ggplot(cv_info, aes(x = replicates, y = reduction)) + 
  geom_line() + 
  geom_point() + 
  labs(
    y = expression(paste("Multiplier on ", sigma)),
    x = "Number of 10F-CV Replicates"
  ) +
  theme_bw() + 
  scale_x_continuous(breaks = 1:10)
```

Larger number of replicates tend to have less impact on the standard error. However, if the baseline value of $\sigma$ is impractically large, the diminishing returns on replication may still be worth the extra computational costs. 

To create repeats, `vfold_cv()` is invoked with an additional argument: 

```{r resampling-repeated}
vfold_cv(ames_train, v = 10, repeats = 5)
```

### Leave-one-out cross-validation {-}

One early variation of cross-validation is leave-one-out (LOO) cross-validation where _V_ is the number of data points in the training set. If there are $n$ training set samples, $n$ models are fit using $n-1$ rows of the training set. Each model predicts the single excluded data point. At the end of resampling, the $n$ predictions are pooled to produce a single performance statistic. 

Leave-one-out methods are deprecated in favor of almost any other method. For anything but pathologically small samples, LOO is computationally excessive and may not have the best statistical properties. `rsample` contains a `loo_cv()` function, but these objects are not generally integrated into the broader tidymodels frameworks.  

### _Monte Carlo_ cross-validation {-}

Finally, another variant of _V_-fold cross-validation is _Monte Carlo_ cross-validation (MCCV, @xu2001monte). Like _V_-fold cross-validation, it allocates a fixed proportion of data to the assessment sets. The difference is that, for MCCV, this proportion of the data is randomly selected each time. This results in assessment sets that are not mutually exclusive. To create these resampling objects: 

```{r resampling-mccv}
mc_cv(ames_train, prop = 9/10, times = 20)
```

### Validation sets {#validation}

Previously mentioned in Section \@ref(what-about-a-validation-set), a validation set is a single partition that is set aside to estimate performance (before using the test set). Graphically: 

```{r resampling-validation, echo = FALSE, out.width = '50%'}
if (knitr:::is_html_output()) {
  file.copy("premade/validation.svg", "_book/premade/validation.svg")
  knitr::include_graphics("premade/validation.svg")
} else {
  file.copy("premade/validation.pdf", "_book/premade/validation.pdf")
  knitr::include_graphics("premade/validation.pdf")
}
```

Validation sets are often used when the original pool of data is very large. In this case, a single large partition may be adequate to characterize model performance without having to do multiple iterations of resampling. 

With `rsample`, a validation set is like any other resampling object; this one only has a single iteration^[In essence, a validation set can be considered Monte Carlo cross-validation with a single iteration.]: 


```{r resampling-validation-alt, echo = FALSE, out.width = '45%'}
if (knitr:::is_html_output()) {
  file.copy("premade/validation-alt.svg", "_book/premade/validation-alt.svg")
  knitr::include_graphics("premade/validation-alt.svg")
} else {
  file.copy("premade/validation-alt.pdf", "_book/premade/validation-alt.pdf")
  knitr::include_graphics("premade/validation-alt.pdf")
}
```

To create a validation set object that uses 3/4 of the data for model fitting: 


```{r resampling-validation-split}
set.seed(12)
val_set <- validation_split(ames_train, prop = 3/4)
val_set
```


### Bootstrapping {#bootstrap}

Bootstrap resampling was originally invented as a method for approximating the sampling distribution of statistics whose theoretical properties are intractable [@davison1997bootstrap]. Using it to estimate model performance is a secondary application of the method. 

A bootstrap sample of the training set is a sample that is the same size as the training set but is drawn _with replacement_. This means that the some training set data point are selected multiple times for the analysis set. Because of this, each data point has a `r round((1-exp(-1)) * 100, 1)`% chance of inclusion in the training set _at least once_. The assessment set contains all of the training set samples that were not selected for the analysis set (on average, with `r round((exp(-1)) * 100, 1)`% of the training set). When bootstrapping, the assessment set is often called the "out-of-bag" sample. 

For a training set of 30 samples, a schematic of three bootstrap samples is: 

```{r resampling-bootstraps, echo = FALSE, out.width = '80%'}
if (knitr:::is_html_output()) {
  file.copy("premade/bootstraps.svg", "_book/premade/bootstraps.svg")
  knitr::include_graphics("premade/bootstraps.svg")
} else {
  file.copy("premade/bootstraps.pdf", "_book/premade/bootstraps.pdf")
  knitr::include_graphics("premade/bootstraps.pdf")
}
```
Note that the sizes of the assessment sets vary. 

Using `rsample`: 

```{r resampling-boot-set}
bootstraps(ames_train, times = 5)
```

Bootstrap samples produce performance estimates that have very low variance (unlike cross-validation) but have significant pessimistic bias. This means that, if the true accuracy of a model is 90%, the bootstrap would tend to estimate the value to be comparatively less than 90%. The amount of bias cannot be empirically determined with sufficient accuracy. Additionally, the amount of bias changes over the scale of the performance metric. For example, the bias is likely to be different when the accuracy is 90% versus when it is 70%. 

The bootstrap is also used inside of many models. For example, the random forest model mentioned earlier contained 1,000 individual decision trees. Each tree was the product of a different bootstrap sample of the training set. 

### Rolling forecasting origin resampling {#rolling}

When the data have a strong time component, a resampling method should be able to estimate seasonal and other temporal trends within the data. A technique that randomly samples values from the training set might disrupt the models ability to estimate these patterns. 

Rolling forecast origin resampling [@hyndman2018forecasting] emulates how temporal might be partitioned in practice. We estimate the model with historical data and evaluate it with the most recent data. For this type of resampling, the size of the initial analysis and assessment sets are specified. This is the first iteration of resampling. The second iteration uses the same data sizes but shifts over by a set number of  samples. 

To illustrate, a training set of 15 samples was resampled with an analysis size of eight samples and an assessment set size of three. The second iteration discards the first training set sample and both data sets shift forward by one. This configuration results in five resamples: 

```{r resampling-rolling, echo = FALSE, out.width = '65%'}
if (knitr:::is_html_output()) {
  file.copy("premade/rolling.svg", "_book/premade/rolling.svg")
  knitr::include_graphics("premade/rolling.svg")
} else {
  file.copy("premade/rolling.pdf", "_book/premade/rolling.pdf")
  knitr::include_graphics("premade/rolling.pdf")
}
```

There are a few different configurations of this method: 

 * The analysis set can cumulative grow (as opposed to remaining the same size). After the first initial analysis set, new samples can accrue without discarding the earlier data. 

 * The resamples need not increment by one. For example, for large data sets, the incremental block could be a week or month instead of a day. 

For a year's worth of data, suppose that six sets of 30 day blocks define the analysis set. For assessment sets of 30 days with a 29 day skip, the `rsample` code is: 

```{r resampling-rolling-forcast}
time_slices <- 
  tibble(x = 1:365) %>% 
  rolling_origin(initial = 6 * 30, assess = 30, skip = 29, cumulative = FALSE)

data_range <- function(x) {
  summarize(x, first = min(x), last = max(x))
}

map_dfr(time_slices$splits, ~   analysis(.x) %>% data_range())
map_dfr(time_slices$splits, ~ assessment(.x) %>% data_range())
```



## Estimating performance {#resampling-performance}

Any of these resampling methods can evaluate the modeling process (e.g., preprocessing, model fitting, etc). They do this in an effective way by having different groups of data to train the model and assess a model. To reiterate the process: 

1. During resampling, the analysis set is used to preprocess the data, apply the preprocessing to itself, and use these processed data to fit the model. 

2. The preprocessing statistics produced by the analysis set is applied to the assessment set. The predictions from the assessment set estimate performance.  

This sequence repeats for every resample. If there are _B_ resamples, there are _B_ replicates of each of the performance metrics. The final resampling estimate is the average of these _B_ statistics. If _B_ = 1, as with a validation set, the individual statistics represent overall performance. 

Let's reconsider the previous random forest model contained in the `rf_wflow` object. The `fit_resamples()` function is analogous to `fit()`. Instead of having a `data` argument, `fit_resamples()` has `resamples` (which expects an `rset` object like the ones shown above). The possible interfaces to the function are:  

```{r resampling-usage, eval = FALSE}
model_spec %>% fit_resamples(formula,  resamples, ...)
model_spec %>% fit_resamples(recipe,   resamples, ...)
workflow   %>% fit_resamples(          resamples, ...)
```

There are a number of options, such as: 

 * `metrics`: A metric set of performance statistics to compute. By default, regression models use RMSE and R<sup>2</sup> while classification models compute the area under the ROC curve and overall accuracy. Note that this choice also defines what predictions are produced during the evaluation of the model. For classification, if only accuracy were requested, class probability estimates are not generated for the assessment set (since they are not needed).
 
 * `control`: A list created by `control_resamples()` with various options. 
 
The control functions include: 

 * `verbose`: A logical for printing process. 
 
 * `extract`: A function for retaining objects from each model iteration (discussed below). 
 
 * `save_pred`: A logical for saving the assessment set predictions. 
 
For our example, the predictions are saved in order to visualize the model fit and residuals: 

```{r resampling-cv-ames}
keep_pred <- control_resamples(save_pred = TRUE)

set.seed(130)
rf_res <- 
  rf_wflow %>% 
  fit_resamples(resamples = ames_folds, control = keep_pred)
rf_res
```
```{r resampling-checkpoint, include = FALSE}
lm_wflow <- 
  workflow() %>% 
  add_recipe(ames_rec) %>% 
  add_model(linear_reg() %>% set_engine("lm"))
save(rf_res, lm_wflow, rf_wflow, file = "RData/resampling.RData", version = 2, compress = "xz")
```

The return value is a tibble along with some extra columns: 

 * `.metrics` is a list column of tibbles containing the assessment set performance statistics. 
 
 * `.notes` is another list column of tibbles cataloging any warnings or errors generated during resampling. Note that errors will not stop subsequent execution of resampling. 
 
 * `.prediction` is present when `save_pred = TRUE`. This list column contains tibbles with the out-of-sample predictions. 
 
While these list columns may look daunting, they can be easily reconfigured using `tidyr` or with  convenience functions. For example, to return the performance metrics in a more usable format: 

```{r resampling-cv-stats}
collect_metrics(rf_res)
```

These are the resampling estimates averaged of the individual replicates of the statistics. To get the metrics for each resample, use the option `summarize = FALSE` 

Note that, in the results above, the performance estimates a more realistic than the resubstitution estimates.  

To obtain the assessment set predictions: 

```{r resampling-cv-pred}
assess_res <- collect_predictions(rf_res)
assess_res
```

The prediction column names follow the conventions discussed for `parsnip` models. The observed outcome column always uses the original column name from the source data. The `.row` column is an integer that matches the row of the original training set so that these results can be properly arranged and merged with the original data. 

:::rmdnote
For some resampling methods, such as the bootstrap or repeated cross-validation, there will be multiple predictions per row of the original training set. To obtain summarized values (averages of the replicate predictions) use `collect_predictions(object, summarize = TRUE)`. 
:::

Since this analysis used 10-fold cross-validation, there is one unique prediction for each training set sample. These data can generate helpful plots of the model to understand where it potentially failed. For example, the observed and predicted values are compared: 

```{r resampling-cv-pred-plot, fig.height=5, fig.width=5}
assess_res %>% 
  ggplot(aes(x = Sale_Price, y = .pred)) + 
  geom_point(alpha = .15) +
  geom_abline(col = "red") + 
  coord_obs_pred() + 
  ylab("Predicted")
```

There was one house in the training set with a low observed sale price that is significantly overpredicted by the model. Which house was that? 

```{r resampling-investigate}
over_predicted <- 
  assess_res %>% 
  mutate(residual = Sale_Price - .pred) %>% 
  arrange(desc(abs(residual))) %>% 
  slice(1)
over_predicted

ames_train %>% 
  slice(over_predicted$.row) %>% 
  select(Gr_Liv_Area, Neighborhood, Year_Built, Bedroom_AbvGr, Full_Bath)
```

These results help us investigate why the prediction was poor for this instance. 

What if a validation set was used instead of cross-validation? From our previous `rsample` object:

```{r resampling-val-ames}
val_res <- rf_wflow %>% fit_resamples(resamples = val_set)
val_res

collect_metrics(val_res)
```

These results are also much closer to the test set results than the resubstitution estimates of performance. 

:::rmdnote
In these analyses, the resampling results are very close to the test set results. The two types of estimates tend to be well correlated. However, this could be from random chance. A seed value of 1352 fixed the random numbers before creating the resamples. Try changing this value and re-running the analyses to investigate whether the resampled estimates match the test set results as well.
:::

## Parallel processing {#parallel}

The models created during resampling are independent of one another. As such, these computations are usually labeled as being "embarrassingly parallel"; each model _could_ be fit simultaneously without issues. The `tune` package use the `foreach` package to facilitates parallel computations. These computations could be split across processors on the same computer or across different computers (depending on the chosen technology). 

For computations conducted on a single computer, the number of possible "worker processes" is  determined by the `parallel` package: 

```{r resampling-find-cores}
# The number of physical cores in the hardware:
parallel::detectCores(logical = FALSE)

# The number of possible independent processes that can 
# be simultaneously used:  
parallel::detectCores(logical = TRUE)
```

The different between these two values is related to the computer's processor. For example, most Intel processors use hyper-threading which creates two _virtual cores_ for each physical core. While these extra resources can improve performance, most of the speed-ups produced by parallel processing occur when processing uses fewer than the number of physical cores. 

For `fit_resamples()`, and other functions in `tune`, parallel processing occurs when the user registers a _parallel backend package_. These R packages define how to execute parallel processing. For example, on unix and macOS operating systems one method of splitting computations is by forking threads. To enable this, load the `doMC` package and register the number of parallel cores with `foreach`: 

```{r resampling-mc, eval = FALSE}
# Unix and macOS only
library(doMC)
registerDoMC(cores = 2)

# Now run fit_resamples()...
```

This instructs `fit_resamples()` to run half of the computations on two cores. To reset the computations to  sequential prcoessing: 

```{r resampling-seq, eval = FALSE}
registerDoSEQ()
```
 
Alternatively, a different approach to parallelizing computations uses network sockets. The `doParallel` package enables this method (usable by all operating systems): 

```{r resampling-psock, eval = FALSE}
# All operating systems
library(doParallel)

# Create a cluster object and then register: 
cl <- makePSOCKcluster(2)
registerDoParallel(cl)

# Now run fit_resamples()`...

stopCluster(cl)
```

Another R package that facilitates parallel processing is the `future` package. Like `foreach`, it provides a framework for parallelism. It is used in conjunction with `foreach` via the `doFuture` package. 

:::rmdnote
The R packages with parallel backends for `foreach` start with the prefix `"do"`. 
:::

Parallel processing with `tune` tends to provide linear speed-ups for the first few cores. This means that, with two cores, the computations are twice as fast. Depending on the data and type of model, the linear speedup deteriorates after 4-5 cores. Using more cores will still reduce the time it takes to complete the task; there are just diminishing returns for the additional cores. 

One final note about parallelism. For each of these technologies, the memory requirements multiply for each additional core used. For example, if the current data set is 2 GB in memory and three cores are used, the total memory requirement is 8 GB (2 for each worker process plus the original). Over specifying cores might cause the computations (and the computer) to slow considerably.

@parallel gives a technical overview of these technologies. 


## Saving the resampled objects {#extract}

While the models created during resampling are not retained, there is a method for keeping them or some of their components. The `extract` option of `control_resamples()` specifies a function that takes a single argument (we'll use `x`). When executed, `x` results in a fitted workflow object (regardless of whether you provided `fit_resamples()` with a workflow). Recall that the workflows package has functions that can pull the different components of the objects (e.g. the model, recipe, etc.). 

Let's fit a linear regression model using the recipe shown at the end of Chapter \@ref(recipes):

```{r resampling-lm-ames}
ames_rec <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
           Latitude + Longitude, data = ames_train) %>%
  step_other(Neighborhood, threshold = 0.01) %>% 
  step_dummy(all_nominal()) %>% 
  step_interact( ~ Gr_Liv_Area:starts_with("Bldg_Type_") ) %>% 
  step_ns(Latitude, Longitude, deg_free = 20)

lm_wflow <-  
  workflow() %>% 
  add_recipe(ames_rec) %>% 
  add_model(linear_reg() %>% set_engine("lm")) 

lm_fit <- lm_wflow %>% fit(data = ames_train)

# Select the recipe: 
pull_workflow_prepped_recipe(lm_fit)
```

Let's save the linear model coefficients for each of the 10 resampled fits: 

```{r resampling-extract-func}
get_model <- function(x) {
  pull_workflow_fit(x) %>% tidy()
}

# Test it using: 
# get_model(lm_fit)
```

The results of the extraction function is wrapped in a list object and returned in a tibble:

```{r resampling-extract-all}
ctrl <- control_resamples(extract = get_model)

lm_res <- lm_wflow %>%  fit_resamples(resamples = ames_folds, control = ctrl)
lm_res
```  

Now there is a `.extracts` column with nested tibbles. What do these contain? 

```{r resampling-extract-res}
lm_res$.extracts[[1]]

# To get the results
lm_res$.extracts[[1]][[1]]
```

This might appear to be a convoluted method for saving the results. However, `extract` is meant to be flexible and does not assume that the user will only save a single tibble per resample. For example, the `tidy()` method might be run on the recipe as well as the model. In this case, the return results is a list of two tibbles. In any case, for this example, all of the results are flattened and collected using:

```{r resampling-extract-fraction}
all_coef <- map_dfr(lm_res$.extracts, ~ .x[[1]][[1]])
# Show the replicates for a single predictor:
filter(all_coef, term == "Year_Built")
```

Chapter \@ref(grid-search) and \@ref(iterative-search) discuss a suite of functions for tuning models. Their interfaces are similar to `fit_resamples()` and many of the features described here apply to those functions.  

## Chapter summary {#resampling-summary}

This chapter describes one of the fundamental tools of data analysis; the ability of measure the performance and variation in model results. Resampling enables us to determine how well the model works without using the test set. 

An important function from the tune package, called `fit_resamples()`, was introduced. The interface for this function is also used in future chapters that describe model tuning tools. 

The data analysis code, to date, for the Ames data is:

```{r resampling-summary, eval = FALSE}
library(tidymodels)
data(ames)
ames <- mutate(ames, Sale_Price = log10(Sale_Price))

set.seed(123)
ames_split <- initial_split(ames, prob = 0.80, strata = Sale_Price)
ames_train <- training(ames_split)
ames_test  <-  testing(ames_split)

ames_rec <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
           Latitude + Longitude, data = ames_train) %>%
  step_log(Gr_Liv_Area, base = 10) %>% 
  step_other(Neighborhood, threshold = 0.01) %>% 
  step_dummy(all_nominal()) %>% 
  step_interact( ~ Gr_Liv_Area:starts_with("Bldg_Type_") ) %>% 
  step_ns(Latitude, Longitude, deg_free = 20)

lm_model <- linear_reg() %>% set_engine("lm")

lm_wflow <- 
  workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(ames_rec)

lm_fit <- fit(lm_wflow, ames_train)

rf_model <- 
  rand_forest(trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("regression")

rf_wflow <- 
  workflow() %>% 
  add_formula(
    Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
      Latitude + Longitude) %>% 
  add_model(rf_model) 

set.seed(55)
ames_folds <- vfold_cv(ames_train, v = 10)

keep_pred <- control_resamples(save_pred = TRUE)

set.seed(130)
rf_res <- rf_wflow %>% fit_resamples(resamples = ames_folds, control = keep_pred)
```


