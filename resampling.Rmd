```{r resampling-setup, include = FALSE}
knitr::opts_chunk$set(fig.path = "figures/")
library(tidymodels)
library(AmesHousing)
library(doMC)

ames <- make_ames()

set.seed(833961)
ames_split <- initial_split(ames, prob = 0.80, strata = Sale_Price)
ames_train <- training(ames_split)
ames_test  <-  testing(ames_split)

registerDoMC(cores = parallel::detectCores())
```

# Resampling for evaluating performance  {#resampling}



```{r resampling-rand-forest-spec}
rf_model <- 
  rand_forest(trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("regression")

rf_rec <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
           Latitude + Longitude, data = ames_train) %>%
  step_log(Sale_Price, base = 10)

rf_wflow <- 
  workflow() %>% 
  add_recipe(rf_rec) %>% 
  add_model(rf_model)
```

```{r resampling-fit, cache = TRUE}
rf_fit_res <- 
  rf_wflow %>% 
  fit(data = ames_train)
```


```{r resampling-eval-train, cache = TRUE}
check_rf <- function(model, dat) {
  reg_metrics <- metric_set(rmse, rsq)
  
  model %>% 
    predict(dat) %>% 
    bind_cols(
      dat %>% 
        select(Sale_Price) %>% 
        mutate(Sale_Price = log10(Sale_Price))
    ) %>% 
    reg_metrics(Sale_Price, .pred)
}
check_rf(rf_fit_res, ames_train)
```


```{r resampling-eval-test, cache = TRUE}
check_rf(rf_fit_res, ames_test)
```

2-fold off of test set. 

Why not re-predict the training set

don't want to go to a test set. 



## Resampling methods

```{r resampling-scheme, echo = FALSE, out.width = '85%'}
if (knitr:::is_html_output()) {
  file.copy("premade/resampling.svg", "_book/premade/resampling.svg")
  knitr::include_graphics("premade/resampling.svg")
} else {
  file.copy("premade/resampling.pdf", "_book/premade/resampling.pdf")
  knitr::include_graphics("premade/resampling.pdf")
}
```

The next three section provide details on three important resampling methods. 


### Cross-validation {#cv}

Cross-validation is an old resampling technique. While there are a number of variations, the most common cross-validation method is called _V_-fold cross-validation. In this case, the data are randomly partitioned into _V_ sets of roughly equal size (called the "folds"). For illustration, _V_ = 3 is shown below for a data set of thirty training set points and a completely random fold allocation. 

```{r resampling-three-cv, echo = FALSE, out.width = '50%'}
if (knitr:::is_html_output()) {
  file.copy("premade/three-CV.svg", "_book/premade/three-CV.svg")
  knitr::include_graphics("premade/three-CV.svg")
} else {
  file.copy("premade/three-CV.pdf", "_book/premade/three-CV.pdf")
  knitr::include_graphics("premade/three-CV.pdf")
}
```

The number inside the symbols are the sample numbers while the color and shape of the symbols represent their randomly assigned folds. The assignment to folds can be done via random sampling or, as was discussed on Section \@ref(splitting-methods), stratified assignment can be used. 

For 3-fold cross-validation, the three iterations of resampling are illustrated below. The first iteration removes the data for the first fold and fits the model to the remaining two folds. This model is used to predict the data from the first fold and model performance is estimated from these data. On the second iteration, the model is fit not the first and third folds and the second fold is used to compute another set of performance metrics. A similar process is used for the third fold. 

```{r resampling-three-cv-iter, echo = FALSE, out.width = '70%'}
if (knitr:::is_html_output()) {
  file.copy("premade/three-CV-iter.svg", "_book/premade/three-CV-iter.svg")
  knitr::include_graphics("premade/three-CV-iter.svg")
} else {
  file.copy("premade/three-CV-iter.pdf", "_book/premade/three-CV-iter.pdf")
  knitr::include_graphics("premade/three-CV-iter.pdf")
}
```

The result is a collection of three replicates for each of the performance statistics. The final resampling estimate of performance averages each of the _V_ replicates. 

_V_ = 3 is a good choice to illustrate cross-validation but is a poor choice in practice. Values of _V_ are most often 5 or 10; here, we generally prefer 10-fold cross-validation as a default. 

```{block, type = "rmdnote"}
What are the effects of changing _V_? Larger values result in resampling estimates with reduced bias but increased noise. Smaller bias have large bias but better noise. 10-fold is preferred since noise can be reduced by replication, as shown below, but bias cannot. See [Section **3.4**]() of @fes for a longer description. 
```

use ames as example

```{r resampling-ames-cv}
set.seed(1352)
ames_folds <- vfold_cv(ames_train, v = 10)
ames_folds

class(ames_folds)
```

split object `[2K/220]`. 

replication

Due to the central limit theorem, the summary statistics that are produced by each model tend toward a normal distribution. For example, for the RMSE statistic, the mean residual is computed using about `r floor(nrow(ames_train) * .1)` houses when 10-fold cross-validation is used. A collection of ten of these statistics can often be reasonably  approximated using a Gaussian distribution. If this collection has a theoretical standard deviation of $\sigma$, so the standard error is $\sigma/\sqrt{10}$. How would replication affect this? Using 5- and 10-fold cross-validation with $R$ replicates, this plot shows how quickly the standard error^[Even if the statistics truly follow a Gaussian distribution, these are _approximate_ standard errors. As will be discussed in the next chapter, there is a within-replicate correlation that is typical of resampled results. By ignoring this extra component of variation, the simple calculations shown in the plot are over-estimates of the reduction in noise in the standard errors.] decreases with replicates: 

```{r resampling-cv-reduction, echo = FALSE}
cv_info <- 
  tibble(replicates = rep(1:10, 2), V = rep(c(5, 10), each = 10)) %>% 
  mutate(B = V * replicates, reduction = 1/B, V = format(V))

ggplot(cv_info, aes(x = replicates, y = reduction, col = V)) + 
  geom_line() + 
  geom_point() + 
  labs(
    y = expression(paste("Multiplier on ", sigma)),
    x = "Number of CV Replicates"
  ) +
  theme_bw() + 
  theme(legend.position = "top") + 
  scale_x_continuous(breaks = 1:10)
```

Larger number of replicates tend to have less impact on the standard error. If the baseline value of $\sigma$ is impractically large, the diminishing returns on replication may still be worth the extra computational power. 

Other types of rs: loo and mc



### Validation sets {#validation}


```{r resampling-validation, echo = FALSE, out.width = '50%'}
if (knitr:::is_html_output()) {
  file.copy("premade/validation.svg", "_book/premade/validation.svg")
  knitr::include_graphics("premade/validation.svg")
} else {
  file.copy("premade/validation.pdf", "_book/premade/validation.pdf")
  knitr::include_graphics("premade/validation.pdf")
}
```


### Bootstrapping {#bootstrap}

```{r resampling-bootstraps, echo = FALSE, out.width = '90%'}
if (knitr:::is_html_output()) {
  file.copy("premade/bootstraps.svg", "_book/premade/bootstraps.svg")
  knitr::include_graphics("premade/bootstraps.svg")
} else {
  file.copy("premade/bootstraps.pdf", "_book/premade/bootstraps.pdf")
  knitr::include_graphics("premade/bootstraps.pdf")
}
```



### Rolling origin forcasting {#rolling}


```{r resampling-rolling, echo = FALSE, out.width = '75%'}
if (knitr:::is_html_output()) {
  file.copy("premade/rolling.svg", "_book/premade/rolling.svg")
  knitr::include_graphics("premade/rolling.svg")
} else {
  file.copy("premade/rolling.pdf", "_book/premade/rolling.pdf")
  knitr::include_graphics("premade/rolling.pdf")
}
```




## Estimating performance {#resampling-performance}

Any of these resampling methods can be used to effectively measure performance of a modeling process (e.g., pre-processing, model fitting, etc). They do this in an effective way by having different groups of data that are used to fit and assess a model. To reiterate the process: 

1. During resampling, the analysis set is used to pre-process the data using the training set, apply the pre-processing to the training set, and then use these process data to fit the model. 

2. The assessment set is used to estimate performance for all of the preceding estimation steps.  

This sequence is conducted for every resample. If there are _B_ resamples, there are _B_ replicates of each of the performance metrics. The final resampling estimate is the average of these _B_ statistics. If _B_ = 1, as with a validation set, the individual statistic is used to represent overall performance. 

Let's reconsider the random forest model used previously. 

```{r resampling-cv-ames}
keep_pred <- control_resamples(save_pred = TRUE)

set.seed(598)
rf_res <- 
  rf_wflow %>% 
  fit_resamples(resamples = ames_folds, control = keep_pred)
rf_res
```
```{r resampling-checkpoint, include = FALSE}
save(rf_res, file = "RData/rf_resampled.RData", version = 2, compress = "xz")
```


```{r resampling-cv-stats}
collect_metrics(rf_res)
```

```{r resampling-cv-replicates}
collect_metrics(rf_res, summarize = FALSE) %>% 
  select(-.estimator) %>% 
  pivot_wider(id_cols = c(id), names_from = .metric, values_from = .estimate)
```


```{r resampling-cv-pred}
assess_res <- collect_predictions(rf_res)
assess_res
```


```{r resampling-cv-pred-plot}
assess_res %>% 
  ggplot(aes(x = Sale_Price, y = .pred)) + 
  geom_point(alpha = .15) +
  geom_abline(col = "red") + 
  coord_equal() + 
  ylab("Predicted")
```


```{r resampling-val-ames}
set.seed(4290)
val_set <- mc_cv(ames_train, times = 1, prop = 3/4)

val_res <- 
  rf_wflow %>% 
  fit_resamples(resamples = val_set)
val_res

collect_metrics(val_res)
```


```{block, type = "rmdnote"}
In these analyses, the resampling results are very close to the test set results. The two types of estimates tend to be well correlated. However, this could be from random chance. A seed value of 1352 was used to fix the random numbers before creating the resamples. Try changing this value and re-running the analyses to investigate of the resampled estimates match the test set results as well.
```

## Parallel processing {#parallel}

## Saving the resampled objects

 