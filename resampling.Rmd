```{r resampling-setup, include = FALSE}
knitr::opts_chunk$set(fig.path = "figures/")
library(tidymodels)
library(AmesHousing)
library(doMC)
library(kableExtra)

ames <- make_ames()

set.seed(833961)
ames_split <- initial_split(ames, prob = 0.80, strata = Sale_Price)
ames_train <- training(ames_split)
ames_test  <-  testing(ames_split)

registerDoMC(cores = parallel::detectCores())
```

# Resampling for evaluating performance  {#resampling}

which metric vs how estimated

advocated data splitting and test set but...


## The resubstitution approach

The Ames data will be used once again to demonstrate the concepts in this chapter. From Chapter \@ref(recipes), a useful recipe for these data was:  

```{r resampling-ames-rec}
# First make a very basic recipe that will be used by multiple models
basic_rec <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
           Latitude + Longitude, data = ames_train) %>%
  step_log(Sale_Price, base = 10)

# Add to the basic recipe to re-create the Ames recipe from Chapter TODO
ames_rec <- 
  basic_rec %>%
  step_log(Gr_Liv_Area, base = 10) %>% 
  step_other(Neighborhood, threshold = 0.01) %>% 
  step_dummy(all_nominal()) %>% 
  step_interact(~ Gr_Liv_Area:starts_with("Bldg_Type_")) %>% 
  step_ns(Latitude, Longitude, deg_free = 20)
```

Using the training and test set split developed in Chapter \@ref(splitting), an ordinary linear regression model is fit to the training set contained in the data frame `ames_train` using a workflow object: 

```{r resampling-lm}
# Fit a simple linear regression model using this recipe. 
lm_fit <- 
  workflow() %>% 
  add_recipe(ames_rec) %>% 
  add_model(linear_reg() %>% set_engine("lm")) %>% 
  fit(data = ames_train)
```

A different type of model is also fit to these data. _Random forests_ is a tree ensemble method that creates a large number of decision trees that are somewhat different. This collection of trees make up the ensemble. When making predictions, a new sample is predicted by each ensemble member and these predictions are averaged and this average is the final ensemble prediction. 

Random forest models are very powerful; these have been shown to be able to emulate the underlying data patterns very closely. While this model can be computationally intensive it is very low-maintainence. Very little pre-processing is required (as documented in Appendix \@ref(pre-proc-table)).

Using the same predictor set as the previous linear model (without the extra pre-processing steps), the random forest model is also fit to the training set (using the underlying `ranger` package): 

```{r resampling-rand-forest-spec}
rf_model <- 
  rand_forest(trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("regression")

rf_fit <- 
  workflow() %>% 
  add_recipe(basic_rec) %>% 
  add_model(rf_model) %>% 
  fit(data = ames_train)
```

There are two models to compare now. How should this be conducted? For demonstration, the training set is predicted to produce what is known as the "apparent error rate" or the  "resubstitution error rate". A function is used to make these predictions and to used some minimal formatting of the results: 

```{r resampling-eval-func}
estimate_perf <- function(model, dat) {
  # Capture the names of the objects used
  cl <- match.call()
  obj_name <- as.character(cl$model)
  data_name <- as.character(cl$dat)
  data_name <- gsub("ames_", "", data_name)
  
  # Estimate these metrics:
  reg_metrics <- metric_set(rmse, rsq)
  
  model %>% 
    predict(dat) %>% 
    bind_cols(
      dat %>% 
        select(Sale_Price) %>% 
        mutate(Sale_Price = log10(Sale_Price))
    ) %>% 
    reg_metrics(Sale_Price, .pred) %>% 
    select(-.estimator) %>% 
    mutate(object = obj_name, data = data_name)
}
```

Both RMSE and R<sup>2</sup> are computed for the data set. For both models, the resubstitution results are: 

```{r resampling-eval-train}
estimate_perf(rf_fit, ames_train)
estimate_perf(lm_fit, ames_train)
```

```{r resampling-eval-train-results, include = FALSE}
all_res <- 
  bind_rows(
    estimate_perf(lm_fit, ames_train),
    estimate_perf(rf_fit, ames_train),
    estimate_perf(lm_fit, ames_test),
    estimate_perf(rf_fit, ames_test)
  ) %>% filter(.metric == "rmse") %>% 
  select(-.metric) %>% 
  pivot_wider(id_cols = object,
              values_from = ".estimate",
              names_from = "data")

tr_ratio <- round(all_res$train[1]/all_res$train[2], 2)
```

From these results, the random forest is much more capable of predicting the sale prices with a RMSE estimate that is `r tr_ratio`-fold better than linear regression. If these were the models that were investigated, the random forest model would probably be chosen to be the final model. The next step would be to apply it to the test set to get a final verification how well the model functions:

```{r resampling-eval-test-rf, cache = TRUE}
estimate_perf(rf_fit, ames_test)
```

The test set RMSE estimate, `r all_res %>% filter(object == "rf_fit") %>% pull("test")`, is much worse than the training set value of `r all_res %>% filter(object == "rf_fit") %>% pull("train")`. Why did this happen? 

Many predictive models are very capable of representing complex trends in the data. In statistics, these are commonly referred to as _low bias models_. 

```{block2, type = "rmdnote"}
In this context, _bias_ is the different between the true data pattern and the types of patterns that the model can emulate. Many black-box machine learning models are low bias. Other models (such as linear/logistic regression, discriminant analysis, and others) are not as adaptable and would be considered _high-bias_ models. See Section [1.2.5](https://bookdown.org/max/FES/important-concepts.html#model-bias-and-variance) of @fes for a discussion.
```

For a low-bias techniques, model complexity can sometimes result in the model to nearly memorizing the training set data. As an obvious example, consider a 1-nearest neighbor model -- it will always provide perfect predictions for the training set no matter how well the model truly works on other data sets. Random forest effectively has the same characteristic; re-predicting the training set will always result in an artificially optimistic estimate of performance.  

For both fitted models, this table summarizes the RMSE estimate for both data sets: 

```{r resampling-rmse-table, echo = FALSE, results = "asis"}
all_res %>% 
  mutate(object = paste0("<tt>", object, "</tt>")) %>% 
  kable(escape = FALSE) %>% 
  kable_styling(full_width = FALSE) %>% 
  add_header_above(c(" ", "RMSE Estimates" = 2))
```

Notice that the linear regression model did not have the same issue since the terms in the model are not highly complex. This model _could_ be made to be low-bias by adding more complex model terms that are appropriate for the data^[It is possible to show that a linear model can memorize the training set. In the `ames_rec` object, change the number of spline terms for `longitude` and `latitude` to a large number (say 1000). This would produce a model fit with a very small resubstitution RMSE and a test set RMSE that is much larger.]. The main take-away from this example is that re-predicting the training set is a bad idea for most models. 

If the test set should not be used immediately and re-predicting the training set is a bad idea, what alternative is there for evaluating the models as they are being developed?  The answer is _resampling methods_, which includes techniques such as cross-validation or validation sets. 


## Resampling methods

Resampling methods can be thought of as empirical simulation systems. They take slightly different versions of the training set, fit the model, then compute performance on data that was not used for the fit. Resampling methods do this repeatedly and build as set of replicated performance statistics. The final resampling estimates of performance are averages of these replicates. 

This diagram illustrates how data are used by resampling methods.

```{r resampling-scheme, echo = FALSE, out.width = '85%'}
if (knitr:::is_html_output()) {
  file.copy("premade/resampling.svg", "_book/premade/resampling.svg")
  knitr::include_graphics("premade/resampling.svg")
} else {
  file.copy("premade/resampling.pdf", "_book/premade/resampling.pdf")
  knitr::include_graphics("premade/resampling.pdf")
}
```

Importantly note that resampling is only conducted on the training set. The test set is not involved. For a specific iteration of resampling, the data are partitioned into two subsamples: 

 * The **analysis set** is used to fit the model. 

 * The **assessment set** is used to evaluate the model. 

These are somewhat analogous to training and test sets. Our language of _analysis_ and _assessment_ data are used to avoid confusion with other splits of the data. These data sets are mutually exclusive. The partitioning scheme used to create the analysis and assessment sets are the main characteristic that defines a particular resampling method. 

Similar partitions are conducted for each iteration of resampling. Suppose 20 iterations of resampling are conducted and RMSE is used to measure model effectiveness. Each assessment set yields its own RMSE statistics and the overall resampling estimate for the model would be the average of the 20 replicate RMSE statistics. 

The next section defines and discussed the most often used methods and their pros and cons. 

### Cross-validation {#cv}

Cross-validation is an old resampling technique. While there are a number of variations, the most common cross-validation method is called _V_-fold cross-validation. In this case, the data are randomly partitioned into _V_ sets of roughly equal size (called the "folds"). For illustration, _V_ = 3 is shown below for a data set of thirty training set points and a completely random fold allocation. 

```{r resampling-three-cv, echo = FALSE, out.width = '50%'}
if (knitr:::is_html_output()) {
  file.copy("premade/three-CV.svg", "_book/premade/three-CV.svg")
  knitr::include_graphics("premade/three-CV.svg")
} else {
  file.copy("premade/three-CV.pdf", "_book/premade/three-CV.pdf")
  knitr::include_graphics("premade/three-CV.pdf")
}
```

The number inside the symbols are the sample numbers while the color and shape of the symbols represent their randomly assigned folds. The assignment to folds can be done via random sampling or, as was discussed on Section \@ref(splitting-methods), stratified assignment can be used. 

For 3-fold cross-validation, the three iterations of resampling are illustrated below. The first iteration removes the data for the first fold and fits the model to the remaining two folds. This model is used to predict the data from the first fold and model performance is estimated from these data. On the second iteration, the model is fit not the first and third folds and the second fold is used to compute another set of performance metrics. A similar process is used for the third fold. 

```{r resampling-three-cv-iter, echo = FALSE, out.width = '70%'}
if (knitr:::is_html_output()) {
  file.copy("premade/three-CV-iter.svg", "_book/premade/three-CV-iter.svg")
  knitr::include_graphics("premade/three-CV-iter.svg")
} else {
  file.copy("premade/three-CV-iter.pdf", "_book/premade/three-CV-iter.pdf")
  knitr::include_graphics("premade/three-CV-iter.pdf")
}
```

For this method and when _V_ = 3, the analysis sets are 2/3 of the training set and each assessment set is a distinct 1/3. The result is a collection of three replicates for each of the performance statistics. The final resampling estimate of performance averages each of the _V_ replicates. 

_V_ = 3 is a good choice to illustrate cross-validation but is a poor choice in practice. Values of _V_ are most often 5 or 10; here, we generally prefer 10-fold cross-validation as a default. 

```{block, type = "rmdnote"}
What are the effects of changing _V_? Larger values result in resampling estimates with reduced bias but increased noise. Smaller bias have large bias but better noise. 10-fold is preferred since noise can be reduced by replication, as shown below, but bias cannot. See [Section **3.4**]() of @fes for a longer description. 
```

For create a set of cross-validation folds, the `vold_cv()` function in `rsample` is used. It takes the data frame containing the training set as well as any options specified by the user: 

```{r resampling-ames-cv}
set.seed(1352)
ames_folds <- vfold_cv(ames_train, v = 10)
ames_folds

class(ames_folds)
```

The `splits` column is a list object of `rsplit` objects that define the partition into the analysis and assessment sets. The print method inside of the tibble shows the frequency of each: `[2K/220]` indicates that roughly two thousand samples are in the analysis set and 220 are in that particular assessment set. 

To manually retrieve the partitioned data, the `analysis()` and `assessment()` functions are used to return a data frame with the appropriate rows for that partition: 

```{r resampling-analysis}
ames_folds$splits[[1]] %>% analysis() %>% dim()
```

tidymodels packages, such as `tune` contain high-level user interfaces so that these functions are not generally needed for day-to-day work. One function to fit a model over these resamples is shown in the next section. 

The `rsample` objects also always contain a character column called `id` with a description of the partition. For some resampling methods, multiple `id` fields are needed.  

There are a variety of variations of _V_-fold cross-validation. The most important is called _repeated_ cross-validation. Depending on the size or other characterisics of the data, the resampling estimate produced by _V_-fold cross-validation may be excessively noisy^[For more details, see [Section 3.4.6](https://bookdown.org/max/FES/resampling.html#resample-var-bias) of @fes.]. As with many statistical problems, one way to reduce noise is to gather more data. For cross-validation, this means averaging more than _V_ statistics. 

To create _R_ repeats of _V_-fold cross-validation, the same partitioning process is redone _R_ times to generate _R_ sets of _V_ partitions. Now, instead of averaging _V_ statistics, $V \times R$ are used to produce the final resampling estimate.

Due to the central limit theorem, the summary statistics that are produced by each model tend toward a normal distribution. For example, for the RMSE statistic, the mean residual is computed using about `r floor(nrow(ames_train) * .1)` houses when 10-fold cross-validation is used. A collection of ten of these statistics can often be reasonably  approximated using a Gaussian distribution. If this collection has a theoretical standard deviation of $\sigma$, so the standard error is $\sigma/\sqrt{10}$. How would replication affect this? Using 10-fold cross-validation with $R$ replicates, this plot shows how quickly the standard error^[Even if the statistics truly follow a Gaussian distribution, these are _approximate_ standard errors. As will be discussed in the next chapter, there is a within-replicate correlation that is typical of resampled results. By ignoring this extra component of variation, the simple calculations shown in the plot are over-estimates of the reduction in noise in the standard errors.] decreases with replicates: 

```{r resampling-cv-reduction, echo = FALSE}
cv_info <- 
  tibble(replicates = rep(1:10, 2), V = 10) %>% 
  mutate(B = V * replicates, reduction = 1/B, V = format(V))

ggplot(cv_info, aes(x = replicates, y = reduction)) + 
  geom_line() + 
  geom_point() + 
  labs(
    y = expression(paste("Multiplier on ", sigma)),
    x = "Number of 10F-CV Replicates"
  ) +
  theme_bw() + 
  scale_x_continuous(breaks = 1:10)
```

Larger number of replicates tend to have less impact on the standard error. If the baseline value of $\sigma$ is impractically large, the diminishing returns on replication may still be worth the extra computational power. 

To create repeats, `vfold_cv()` is invoked with another argument: 

```{r resampling-repeated}
vfold_cv(ames_train, v = 10, repeats = 5)
```

One early variation of cross-validation is leave-one-out (LOO) cross-validation where _V_ is the number of data points in the training set. If there are $n$ training set samples, $n$ models are created (using $n-1$ rows of the training set). For each, the single data point that was excluded is predicted. At the end of resampling, this collection of $n$ predictions are pooled and a single set of performance metrics are generated (as opposed to averaging _V_ replicates). 

Leave-one-out methods are fairly deprecated in favor of almost any other method. For anything but small samples, LOO is computationally excessive and may not have the best statistical properties. `rsample` contains a `loo_cv()` function, but these objects are not generally integrated into the broader tidymodels frameworks.  

Finally, another variant of _V_-fold cross-validation is called _Monte Carlo_ cross-validation (MCCV). Like _V_-fold cross-validation, it leaves out $1/V$ of the data out at each iteration. The difference is that, for MCCV, this proportion of the data is chosen at random each time. While it leaves the same amount in the analysis and assessment sets as _V_-fold cross-validation, the assessment sets are not mutually exclusive. To create these resampling objects: 

```{r resampling-mccv}
mc_cv(ames_train, prop = 9/10, times = 20)
```

### Validation sets {#validation}

Previously mentioned in Section \@ref(what-about-a-validation-set), this is a single partition that is set aside to be used to estimate performance (before using the test set). Graphically: 

```{r resampling-validation, echo = FALSE, out.width = '50%'}
if (knitr:::is_html_output()) {
  file.copy("premade/validation.svg", "_book/premade/validation.svg")
  knitr::include_graphics("premade/validation.svg")
} else {
  file.copy("premade/validation.pdf", "_book/premade/validation.pdf")
  knitr::include_graphics("premade/validation.pdf")
}
```

Validation sets are often used when the original pool of data is very large. In this case, a single large partition may be adequate to characterize model performance without having to do multiple iterations of resampling. 

With `rsample`, a validation set is treated like any other resampling object; this one only has a single iteration^[In essence, a validation set can be considered a single Monte Carlo cross-validation with a single iteration.]: 


```{r resampling-validation-alt, echo = FALSE, out.width = '45%'}
if (knitr:::is_html_output()) {
  file.copy("premade/validation-alt.svg", "_book/premade/validation-alt.svg")
  knitr::include_graphics("premade/validation-alt.svg")
} else {
  file.copy("premade/validation-alt.pdf", "_book/premade/validation-alt.pdf")
  knitr::include_graphics("premade/validation-alt.pdf")
}
```

To create a validation set object that uses 3/4 of the data for model fitting: 


```{r resampling-validation-split}
set.seed(4290)
val_set <- validation_split(ames_train, prop = 3/4)
val_set
```


### Bootstrapping {#bootstrap}

Bootstrap resampling was originally invented as a method for approximating the sampling distribution of statistics where the theoretical properties are intractable. Using it to estimate model performance is a secondary application of the method. 

A bootstrap samples of the training set is a sample that is the same size as the training set but is drawn _with replacement_. This means that the same training set data point can be selected multiple times for the analysis set. Because of this, each data point has a `r round((1-exp(-1)) * 100, 2)`% chance of being included in the training set at least once. The assessment set contains all of the training set samples that were not selected for the analysis set (on average, with `r round((exp(-1)) * 100, 2)`% of the training set). When boostrapping, the assessment set is also called the "out-of-bag sample". 

For a training set of 30 samples, a schematic of three bootstrap samples is: 

```{r resampling-bootstraps, echo = FALSE, out.width = '80%'}
if (knitr:::is_html_output()) {
  file.copy("premade/bootstraps.svg", "_book/premade/bootstraps.svg")
  knitr::include_graphics("premade/bootstraps.svg")
} else {
  file.copy("premade/bootstraps.pdf", "_book/premade/bootstraps.pdf")
  knitr::include_graphics("premade/bootstraps.pdf")
}
```
Note thast the sizes of the assessment sets can vary. 

Using `rsample`: 

```{r resampling-boot-set}
bootstraps(ames_train, times = 5)
```

Bootstrap samples produce performance estimates that have very low variance (unlike cross-validation) but have significant pessimistic bias. This means that, if the true accuracy of a model is 90%, the bootstrap would tend to estimate the value to be comparatively less than 90%. The amount of bias cannot be accurately estimated and probably changes over the scale of the performance metric. For example, the bias is likely to be different when the accuracy is 90% versus when it is 70%. 

The bootstrap is also used inside of many models. For example, the random forest model mentioned earlier contained 1,000 individual decision trees. Each one was created using a different bootstrap sample of the training set. 

### Rolling origin forcasting {#rolling}

When the data have a strong time component, a resampling method should attempt to be able to estimate seasonal and other temporal trends on the data. A technique that randomly samples values from the training set might disrupt the models ability to estimate these patterns. 

Rolling forecast origin resampling emulates how temporal might be partitioned; historical data would be used to estimate the model and the most recent data would be allocated for evaluation. For this type of resampling, the size of the initial analysis and assessment sets are specified. This is the first iteration of resampling. The second iteration uses the same data sizes but shifts over by one sample. 

To illustrate, a training set of 15 samples is resampled with an analysis size of eight samples and an assessment set size of three. The second iteration discards the first training set sample and both data sets shift forward by one. This configuration results in five resamples: 

```{r resampling-rolling, echo = FALSE, out.width = '65%'}
if (knitr:::is_html_output()) {
  file.copy("premade/rolling.svg", "_book/premade/rolling.svg")
  knitr::include_graphics("premade/rolling.svg")
} else {
  file.copy("premade/rolling.pdf", "_book/premade/rolling.pdf")
  knitr::include_graphics("premade/rolling.pdf")
}
```

There are a few different configurations of this method: 

 * The analysis set can cumulative grow. After the first initial analysis set, new samples are accrued without discarding the earlier data. 

 * The resamples need not increment by one. For example, for large data sets, the incremental block could be a week or month instead of a day. 

For a year's worth of data, suppose that six sets of 30 day blocks were used as the analysis set. For assessment sets of 30 days with a 29 day skip, the `rsample` code is: 

```{r resampling-rolling-forcast}
time_slices <- 
  tibble(x = 1:365) %>% 
  rolling_origin(initial = 6 * 30, assess = 30, skip = 29, cumulative = FALSE)

data_range <- function(x) {
  x %>% 
    summarize(first = min(x), last = max(x))
}

map_dfr(time_slices$splits, ~ analysis(.x) %>% data_range())
map_dfr(time_slices$splits, ~ assessment(.x) %>% data_range())
```



## Estimating performance {#resampling-performance}

Any of these resampling methods can be used to effectively measure performance of a modeling process (e.g., pre-processing, model fitting, etc). They do this in an effective way by having different groups of data that are used to fit and assess a model. To reiterate the process: 

1. During resampling, the analysis set is used to pre-process the data using the training set, apply the pre-processing to the training set, and then use these process data to fit the model. 

2. The assessment set is used to estimate performance for all of the preceding estimation steps.  

This sequence is conducted for every resample. If there are _B_ resamples, there are _B_ replicates of each of the performance metrics. The final resampling estimate is the average of these _B_ statistics. If _B_ = 1, as with a validation set, the individual statistic is used to represent overall performance. 

Let's reconsider the random forest model used previously. 

```{r resampling-cv-ames}
keep_pred <- control_resamples(save_pred = TRUE)

rf_wflow <- 
  workflow() %>% 
  add_recipe(basic_rec) %>% 
  add_model(rf_model) 

set.seed(598)
rf_res <- 
  rf_wflow %>% 
  fit_resamples(resamples = ames_folds, control = keep_pred)
rf_res
```
```{r resampling-checkpoint, include = FALSE}
lm_wflow <- 
  workflow() %>% 
  add_recipe(ames_rec) %>% 
  add_model(linear_reg() %>% set_engine("lm"))
save(rf_res, lm_wflow, rf_wflow, file = "RData/resampling.RData", version = 2, compress = "xz")
```


```{r resampling-cv-stats}
collect_metrics(rf_res)
```

```{r resampling-cv-replicates}
collect_metrics(rf_res, summarize = FALSE) %>% 
  select(-.estimator) %>% 
  pivot_wider(id_cols = c(id), names_from = .metric, values_from = .estimate)
```


```{r resampling-cv-pred}
assess_res <- collect_predictions(rf_res)
assess_res
```


```{r resampling-cv-pred-plot}
assess_res %>% 
  ggplot(aes(x = Sale_Price, y = .pred)) + 
  geom_point(alpha = .15) +
  geom_abline(col = "red") + 
  coord_equal() + 
  ylab("Predicted")
```


```{r resampling-val-ames}
set.seed(4290)
val_set <- mc_cv(ames_train, times = 1, prop = 3/4)

val_res <- 
  rf_wflow %>% 
  fit_resamples(resamples = val_set)
val_res

collect_metrics(val_res)
```


```{block, type = "rmdnote"}
In these analyses, the resampling results are very close to the test set results. The two types of estimates tend to be well correlated. However, this could be from random chance. A seed value of 1352 was used to fix the random numbers before creating the resamples. Try changing this value and re-running the analyses to investigate of the resampled estimates match the test set results as well.
```

## Parallel processing {#parallel}

## Saving the resampled objects

 