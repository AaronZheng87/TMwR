```{r tuning-setup, include = FALSE}
knitr::opts_chunk$set(fig.path = "figures/")
library(tidymodels)
library(patchwork)
library(ggforce)
library(doMC)
registerDoMC(cores = parallel::detectCores())

## -----------------------------------------------------------------------------

source("ames_snippets.R")

## -----------------------------------------------------------------------------

data(two_class_dat)

set.seed(91)
split <- initial_split(two_class_dat)

training_set <- training(split)
testing_set  <-  testing(split)

data_grid <- crossing(A = seq(0.4, 4, length = 200), B = seq(.14, 3.9, length = 200))

## -----------------------------------------------------------------------------

load("RData/search_examples.RData")
```

# Model tuning and the dangers of overfitting {#tuning}

Model parameters are unknown values that require estimation in order to use the model. Using simple linear regression again as the example: 

$$ y_i = \beta_0 + \beta_1 x_i + \epsilon_i$$

Once we have the outcome ($y$) and predictor ($x$) data, the two parameters $\beta_0$ and $\beta_1$ are directly estimated: 

$$\hat \beta_1 = \frac{\sum_i (y_i-\bar{y})(x_i-\bar{x})}{\sum_i(x_i-\bar{x})^2}$$ 

and

$$\hat \beta_0 = \bar{y}-\hat \beta_1 \bar{x}.$$
After parameter estimation, other important qualities (e.g., covariance matrix, confidence intervals, etc.) are capable of being calculated. The fact that we have the ability to directly estimate these values from the data indicates that they are **analytically tractable**. 

There are situations where a model's equation contains unknown parameters that can't be directly estimated from the data. For the $K$-nearest neighbors model, the prediction equation for a new value $x_0$ is

$$\hat y = \frac{1}{K}\sum_{\ell = 1}^K x_\ell^*$$
where $K$ is the number of neighbors and the $x_\ell^*$ are the $K$ closest values to $x_0$ in the training set. 

The model itself is not defined by a model equation; the prediction equation shown above defines it. This factor, and the possible intractability of a the distance measure, makes it impossible to create a set of equations that can solved for $K$ (iteratively or otherwise). The number of neighbors has a profound impact on the model; it governs the flexibility of the class boundary. For small values of $K$, the boundary is very elaborate while large values might be unduly smooth. 

The number of nearest neighbors is a good example of a **tuning parameter**: an unknown structural value that has significant impact on the model but _cannot be directly estimated from the data_. 

This chapter is strategic in nature. The outline is to: 

 * Provide additional examples of tuning parameters. 

 * Demonstrate that poor choices of these values lead to overfitting. 

 * Introduce several tactics for finding optimal tuning parameters values. 
 
 * Present tidymodels functions for tuning parameters. 

Subsequent chapters chapters go into more detail on specific optimization methods. 


## Tuning parameters for different types of models {#tuning-parameter-examples}

Tuning parameters are often found in machine learning and predictive models. Some examples are: 

* Boosting is an ensemble method that combines a series of base models, each of which is created sequentially and depends on the previous models. The number of boosting iterations is an important parameter that usually requires optimization.  

* In the classic single-layer artificial neural network (a.k.a. the multilayer perceptron), the predictors are combined using two or more hidden units. The hidden units are linear combinations of the predictors that are encased in an _activation function_ (typically a nonlinear function, such as a sigmoid). The hidden units are then connected to the outcome units; one outcome unit is used for regression models and multiple outcome units are required for classification. The number of hidden units and the type of activation function are important structural tuning parameters. 

* Modern gradient decent methods are improved by finding the right optimization parameters. Examples are learning rates, momentum, and the number of optimization iterations/epochs [@Goodfellow]. Neural networks and some ensemble models use gradient decent to estimate the model parameters. While the tuning parameters associated with gradient decent are not structural parameters, they often require tuning. 

In some cases, the preprocessing techniques require values: 

* In principal component analysis, or its supervised cousin called partial least squares, the predictors are replaced with new, artificial features that have better properties (related to collinearity). The number of extracted components can be tuned. 

* Imputation methods estimate missing predictor values using the complete values of one or more predictors. One effective imputation tool uses $K$-nearest neighbors of the complete columns to predict the missing value. The number of neighbors modulates the amount of averaging.  

Some classical statistical models also have structural parameters: 

 * In binary regression, the logit link is commonly used (i.e., logistic regression). Other link functions, such as the probit, complementary log-log, are also available [@Dobson99]. This example is described in more detail in the next section. 

 * Non-Bayesian longitudinal and repeated measures models require a specification for the covariance or correlation structure of the data. Options include compound symmetric (a.k.a. exchangeable), autoregressive, Toeplitz and others [@littell2000modelling]. 

A counter example is the prior distribution required for Bayesian analysis. The prior encapsulates the analyst's prior belief regarding the distribution of parameters. For example, in Section \@ref(tidyposterior), a Bayesian ANOVA model was used. For the  regression parameters, we were unclear about what the prior should be (beyond being a symmetric distribution). The chosen prior was a t-distribution with one degree of freedom since it has heavier tails; this reflects our added uncertainty. Our prior beliefs should not be subject to optimization. In many cases, the tuning parameters are optimized for performance whereas the prior should not be tweaked to get "the right results." 

:::rmdnote
While debatable, another counter-example is the number of trees in a random forest or bagging model. This value should be large enough to ensure numerical stability in the results; tuning it cannot improve performance as long as the value is large enough to produce reliable results. For random forest, this value is typically in the thousands while the number of trees needed for bagging is around 50 - 100. 
:::

## What do we optimize? 

How should we evaluate models when we optimize tuning parameters?  It depends on the model and the purpose of the model. 

For cases where the statistical properties of the tuning parameter are tractable, common statistical properties can be used as the objective function. For example, in the case of binary logistic regression, the link function can be chosen by maximizing the likelihood or information criteria. However, these statistical properties may not align with the results achieved using accuracy-oriented properties. As an example,  @FriedmanGFA optimized the number of trees in a boosted tree ensemble and found different when maximizing the likelihood and accuracy:

> "degrading the likelihood by overfitting actually improves misclassification error rate. Although perhaps counterintuitive, this is not a contradiction; likelihood and error rate measure different aspects of fit quality."

To demonstrate, consider the classification data shown below were there are two predictors, two classes, and a training set of `r nrow(training_set)` data points:  

```{r tuning-two-class-dat, echo = FALSE}
ggplot(training_set, aes(x = A, y = B, col = Class)) + 
  geom_point(alpha = .5) + 
  coord_equal() + 
  labs(x = "Predictor A", y = "Predictor B")
```

A reasonable first approach is to fit a linear class boundary to these data. The most common method for doing this is to use a generalized linear model in the form of _logistic regression_. This model relates the _log odds_ of a sample being Class 1 using the _logit_ transformation: 

$$ \log\left(\frac{\pi}{1 - \pi}\right) = \beta_0 + \beta_1x_1 + \ldots + \beta_px_p$$
In the context of generalized linear models, the logit function is the _link function_ between the outcome ($\pi$) and the predictors. There are other link functions that include the _probit_ model: 

$$\Phi^{-1}(\pi) = \beta_0 + \beta_1x_1 + \ldots + \beta_px_p$$
were $\Phi$ is the cumulative standard normal function, and the _complementary log-log_ model

$$\log(−\log(1−\pi)) = \beta_0 + \beta_1x_1 + \ldots + \beta_px_p$$
Each of these models result in linear class boundaries. Which one should be used? Since, for these data, the number of model parameters does not vary, the statistical approach is to compute the (log) likelihood for each model and determine the model with the largest value. Traditionally, the likelihood is computed using the same data that were used to estimate the parameters.

For a data frame `training_set`, a function computes the different models and extract the likelihood statistics for the training set (using `broom::glance()`): 

```{r tuning-likelihood}
llhood <- function(...) {
  logistic_reg() %>% 
    set_engine("glm", ...) %>% 
    fit(Class ~ ., data = training_set) %>% 
    glance() %>% 
    select(logLik)
}

bind_rows(
  llhood(),
  llhood(family = binomial(link = "probit")),
  llhood(family = binomial(link = "cloglog"))
) %>% 
  mutate(link = c("logit", "probit", "c-log-log"))  %>% 
  arrange(desc(logLik))
```

Using these results, the logistic model has the best statistical properties. 

From the scale of the log-likelihood values, it is difficult to understand if these differences are important or negligible.  One way of improving this analysis is the resample the statistics and separate the modeling data from the data used for performance estimation. With this small data set, repeated 10-fold cross-validation is a good choice for resampling. In the `r pkg(yardstick)` package, the `mn_log_loss()` is used to estimate the negative log-likelihood: 

```{r tuning-resampled-log-lhood, fig.height = 3}
set.seed(1292)
rs <- vfold_cv(training_set, repeats = 10)

# Return the individual resampled performance estimates:
lloss <- function(...) {
  perf_meas <- metric_set(roc_auc, mn_log_loss)
    
  logistic_reg() %>% 
    set_engine("glm", ...) %>% 
    fit_resamples(Class ~ A + B, rs, metrics = perf_meas) %>% 
    collect_metrics(summarize = FALSE) %>%
    select(id, id2, .metric, .estimate)
}

resampled_res <- 
  bind_rows(
    lloss()                                    %>% mutate(model = "logitistic"),
    lloss(family = binomial(link = "probit"))  %>% mutate(model = "probit"),
    lloss(family = binomial(link = "cloglog")) %>% mutate(model = "c-log-log")     
  ) %>%
  # Convert log-loss to log-likelihood:
  mutate(.estimate = ifelse(.metric == "mn_log_loss", -.estimate, .estimate)) %>% 
  group_by(model, .metric) %>% 
  summarize(
    mean = mean(.estimate, na.rm = TRUE),
    std_err = sd(.estimate, na.rm = TRUE)/sum(!is.na(.estimate)), 
    .groups = "drop"
  )

resampled_res %>% 
  filter(.metric == "mn_log_loss") %>% 
  ggplot(aes(x = mean, y = model)) + 
  geom_point() + 
  geom_errorbar(aes(xmin = mean - 1.96 * std_err, xmax = mean+ 1.96 * std_err),
                width = .1) + 
  labs(y = NULL, x = "log-likelihood")
```
The scale of these values is different than the previous values since they are computed on a smaller data set; the value produced by `broom::glance()` is a sum while `yardstick::mn_log_loss()` is an average.  

From these results, there is considerable evidence that _the choice of the link function matters_ and that the logistic model is superior. 

What about a different metric? The area under the ROC curve was also calculated for each resample. These results, which reflects the discriminative ability of the models across numerous probability thresholds, show _a lack of differences_: 

```{r tuning-resampled-roc, echo = FALSE, fig.height = 3}
resampled_res %>% 
  filter(.metric == "roc_auc") %>% 
  ggplot(aes(x = mean, y = model)) + 
  geom_point() + 
  geom_errorbar(aes(xmin = mean - 1.96 * std_err, xmax = mean+ 1.96 * std_err),
                width = .1) + 
  labs(y = NULL, x = "area under the ROC curve")
```

Given the overlap of the intervals, as well as the scale of the x-axis, any of these options could be used. This is also reinforced when the class boundaries for the three models are overlaid on the _test set_ of `r nrow(testing_set)` data points: 

```{r tuning-glm-fits, echo = FALSE}
logit_pred <- 
  logistic_reg() %>% 
  set_engine("glm") %>% 
  fit(Class ~ A + B, data = training_set) %>% 
  predict(data_grid, type = "prob") %>% 
  bind_cols(data_grid) %>% 
  mutate(link = "logit")

probit_pred <- 
  logistic_reg() %>% 
  set_engine("glm", family = binomial(link = "probit")) %>% 
  fit(Class ~ A + B, data = training_set) %>% 
  predict(data_grid, type = "prob") %>% 
  bind_cols(data_grid) %>% 
  mutate(link = "probit")

cloglog_pred <- 
  logistic_reg() %>% 
  set_engine("glm", family = binomial(link = "cloglog")) %>% 
  fit(Class ~ A + B, data = training_set) %>% 
  predict(data_grid, type = "prob") %>% 
  bind_cols(data_grid) %>% 
  mutate(link = "c-log-log")

link_grids <- 
  bind_rows(logit_pred, probit_pred, cloglog_pred) %>% 
  mutate(link = factor(link, levels = c("logit", "probit", "c-log-log")))

link_grids %>% 
  ggplot(aes(x = A, y = B)) + 
  geom_point(data = testing_set, aes(col = Class), alpha = .5, show.legend = FALSE) + 
  geom_contour(aes( z = .pred_Class1, lty = link), breaks = 0.5, col = "black") + 
  coord_equal() + 
  labs(x = "Predictor A", y = "Predictor B")
```

The point of this exercise is to emphasize that different metrics might lead to different decisions about the choice of tuning parameter values. In this case, one metric appear clearly sort the models while another metric shows no difference.  

Metric optimization is thoroughly discussed by @thomas2020problem who discuss several issues, including the gaming of metrics. They warn that: 

> The unreasonable effectiveness of metric optimization in current AI approaches is a fundamental challenge to the field, and yields an inherent contradiction: solely optimizing metrics leads to far from optimal outcomes.

As shown above, we can collect multiple metrics and make decisions on a variety of aspects of model effectiveness. 

## The consequences of poor parameter estimates {#overfitting-bad}

Many tuning parameters modulate the amount of model complexity. Complexity often implies a great deal of malleability in the patterns that the model can emulate. For example, as shown in Section \@ref(other-steps), adding degrees of freedom in a spline function increases the intricacy of the prediction equation. While this is an advantage when the underlying motifs in the data are complex, it can also lead to over interpretation of chance patterns that would not reproduce in new data. _Overfitting_ is the situation where a model adapts too much to the training data; it performance well on the data used to build the model and does poorly for new data. Since tuning patterns enable complexity, poor choices can lead to overfitting. 

Recall the single layer neural network model described in Section \@ref(tuning-parameter-examples). With a single hidden unit and sigmoidal activation functions a neural network for classification is, for all intents and purposes, logistic regression. However, as the number of hidden units increases, so does the allowable complexity of the model. In fact, when the network model uses sigmoidal activation units, @cybenko1989approximation showed that the model is a universal function approximator as long as there are enough hidden units.

Using the two-class data from the previous section, neural network classification models were fit to the data with varying number of hidden units. Using the area under the ROC curve as a performance metric, the effectiveness of the model on the training set increases as mode hidden units are added. The network model is thoroughly learning the training set. If the model judges itself on the training set ROC value, it prefers many hidden units so that it can nearly eliminate errors. 


```{r tuning-two-class-comps, include = FALSE, cache = TRUE}
two_class_rec <-
 recipe(Class ~ ., data = two_class_dat) %>% 
 step_normalize(all_predictors()) 

mlp_mod <- 
 mlp(hidden_units = tune(), epochs = 1000) %>% 
 set_engine("nnet") %>%
 set_mode("classification")

mlp_wflow <- 
 workflow() %>% 
 add_recipe(two_class_rec) %>% 
 add_model(mlp_mod)

mlp_res <-
 tibble(
  hidden_units = 1:20,
  train = NA_real_,
  test = NA_real_,
  model = vector(mode = "list", length = 20)
 )

for(i in 1:nrow(mlp_res)) {
  set.seed(27)
 tmp_mod <-
  mlp_wflow %>% finalize_workflow(mlp_res %>% slice(i) %>% select(hidden_units)) %>%
  fit(training_set)
 mlp_res$train[i] <-
  roc_auc_vec(training_set$Class, predict(tmp_mod, training_set, type = "prob")$.pred_Class1)
 mlp_res$test[i]  <-
  roc_auc_vec(testing_set$Class, predict(tmp_mod, testing_set, type = "prob")$.pred_Class1)
 mlp_res$model[[i]] <- tmp_mod
}
```

It was shown in Chapters \@ref(splitting) and \@ref(resampling) that simply repredicting the training set is a poor approach to model evaluation. Here, the neural networks very quickly begin to overinterpret patterns that is sees there. Here are three example class boundaries developed with the training set and are overlaid on training and test sets: 

```{r tuning-two-class-boundaries, echo = FALSE, fig.height = 8}

te_plot <- 
  mlp_res %>% 
  slice(c(1, 4, 20)) %>% 
  mutate(
    probs = map(model, ~ bind_cols(data_grid, predict(.x, data_grid, type = "prob")))
  ) %>% 
  dplyr::select(hidden_units, probs) %>% 
  unnest(cols = c(probs)) %>% 
  mutate(
    label = paste(format(hidden_units), "units"),
    label = ifelse(label == " 1 units", " 1 unit", label)
  ) %>% 
  ggplot(aes(x = A, y = B)) + 
  geom_point(data = testing_set, aes(col = Class), alpha = .5, show.legend = FALSE) + 
  geom_contour(aes( z = .pred_Class1), breaks = 0.5, col = "black") + 
  facet_wrap(~ label, nrow = 1) + 
  coord_equal() + 
  ggtitle("Test Set") + 
  labs(x = "Predictor A", y = "Predictor B")

tr_plot <- 
  mlp_res %>% 
  slice(c(1, 4, 20)) %>% 
  mutate(
    probs = map(model, ~ bind_cols(data_grid, predict(.x, data_grid, type = "prob")))
  ) %>% 
  dplyr::select(hidden_units, probs) %>% 
  unnest(cols = c(probs)) %>% 
  mutate(
    label = paste(format(hidden_units), "units"),
    label = ifelse(label == " 1 units", " 1 unit", label)
  ) %>% 
  ggplot(aes(x = A, y = B)) + 
  geom_point(data = training_set, aes(col = Class), alpha = .5, show.legend = FALSE) + 
  geom_contour(aes( z = .pred_Class1), breaks = 0.5, col = "black") + 
  facet_wrap(~ label, nrow = 1) + 
  coord_equal() + 
  ggtitle("Training Set") + 
  labs(x = "Predictor A", y = "Predictor B")

tr_plot / te_plot


```

The single unit model does not adapt very well to the model (since it is constrained to be linear). A model containing four units begins to show signs of overfitting with an unrealistic boundary for values away from the data mainstream. This is caused by a single data point from the first class in the upper right corner of the data.  By 20 units, the model is beginning to memorize the training set, forming small islands around those data (to minimize the resubstitution error rate). These patterns do not repeat in the test set. This last panel is the best illustration of how tuning parameters that control complexity must be modulated so that the model is effective. For a 20 unit model, the training set ROC AUC is `r round(mlp_res$train[20], 3)` but the test set value is `r round(mlp_res$test[20], 3)`. 

The occurrence of overfitting is obvious with two predictors. However, in general, we require a quantitative means for detecting overfitting. As was just shown, using out-of-sample data is the solution for detecting when the model is overemphasizing the training set. Rather than using the test set, some form of resampling  is required. This could mean an iterative approach (e.g., 10-fold cross-validation) or a single data source (e.g., a validation set). 

## Two general strategies for optimization

Tuning parameter optimization usually fall into one of two categories: pre-defining a set of parameter values to evaluate or to sequentially discover new parameter combinations based on previous results. 

The use of pre-defined sets is commonly called grid search. The main choices for grid search are how to make the grid and how many parameter combinations to evaluate. Grid search is often thought to be inefficient since, with many tuning parameters, the number of grid points required to cover the parameter space is large (due to the curse of dimensionality). While there is truth to this claim, it is most true when the process is not optimized. This is discussed more in Chapter \@ref(grid-search).

For sequential search methods, almost any nonlinear optimization method is appropriate, although some are more efficient than others. In some cases, an initial set of results for one or more parameter combinations is required to start the optimization process. 

The plot below shows two panels that demonstrate these approaches when there are two tuning parameters that range between zero and one. In each, a set of blue contours show the true (simulated) relationship between the parameters and the outcome. The optimal results are in the upper right-hand corner. 

```{r tuning-strategies, echo = FALSE}
grid_plot <-
  ggplot(sfd_grid, aes(x = x, y = y)) +
  geom_point() +
  lims(x = 0:1, y = 0:1) +
  labs(x = "Parameter 1", y = "Parameter 2", title = "Space-Filling Grid") +
  geom_contour(data = grid_contours,
               aes(z = obj),
               alpha = .3,
               bins = 12) +
  coord_equal() +
  theme_bw() +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())

search_plot <-
  ggplot(nm_res, aes(x = x, y = y)) +
  geom_point(size = .7)  +
  lims(x = 0:1, y = 0:1) +
  labs(x = "Parameter 1", y = "Parameter 2", title = "Global Search") +
  coord_equal()  +
  geom_contour(data = grid_contours,
               aes(x = x, y = y, z = obj),
               alpha = .3,
               bins = 12) +
  theme_bw() +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())

grid_plot + search_plot
```

The left-hand panel shows a type of grid called a space-filling design. This is a type of experimental design devised for covering the parameter space such that tuning parameter combinations that are not close to one another. The results for this design does not place points exactly at the truly optimal location. However, one point is in the general vicinity and would probably have performance metric results that are within the noise of the absolute optimal value. 

The right-hand panel illustrates the results of a global search method: the Nelder-Mead simplex method [@Olsson:1975p3609]. The starting point is in the lower-left part of the parameter space. The search meanders across the space until it reaches the optimum location, where it strives to come as close as possible to the numerically best value. This particular search method, while effective, is not known for its efficiency; it requires many function evaluations, especially near the optimal values. In Chapter \@ref(iterative-search), more efficient search tools are discussed. 

Hybrid strategies are also an option. After an initial grid search, a sequential optimization starts from the best grid combination.

Examples of these strategies are discussed in the detail in the two subsequent chapters. Before moving on, the `r pkg(dials)` package is introduced. It is focused on working with tuning parameter objects. 

## Tuning parameters in tidymodels

In our previous chapters, recipes and model specifications contained several function arguments that corresponded to tuning parameters: 

* The threshold for combining neighborhoods into an "other" category (with argument name `threshold`) in Section \@ref(dummies). 

* The number of degrees of freedom in a natural spline (`deg_free`, Section \@ref(other-steps)). 

* The number of data points to required to execute a split in a tree-based model (`min_n`, Section \@ref(models)). 

* The amount of regularization in penalized models (`penalty`, Section \@ref(models)).  

For `r pkg(parsnip)` model specifications, parameter arguments occur in two locations. **Main arguments** are those that are most often optimized for performance and are available in multiple engines. The main tuning parameters are top-level arguments to the model specification function. For example, the `rand_forest()` function has main arguments `trees`, `min_n`, and `mtry` since these are most frequently specified or optimized. 

A secondary set of tuning parameters are **engine-specific**. These are either infrequently optimized or are only specific to certain engines. Again, with random forests, the `r pkg(ranger)` package contains some arguments that other not used by other packages. One example is _gain penalization_, which regularizes the predictor selection in the tree induction process. This parameter can help modulate the trade-off between the number of predictors used in the ensemble and performance. @wundervald2020generalizing. The name of this argument in  `ranger()` is `regularization.factor`. To specify a value via a `r pkg(parsnip)` model specification, it is added as a supplemental argument to `set_engine()`: 

```{r tuning-args, eval = FALSE}
rand_forest(trees = 2000, min_n = 10) %>%                   # <- main arguments
  set_engine("ranger", regularization.factor = 0.5)         # <- engine-specific
```

Note that the main arguments use a  "harmonized"  naming system to remove inconsistencies across engines while engine-specific arguments do not. 

How can we signal to tidymodels functions which arguments should be optimized?  Parameters are marked for tuning by giving them a value of `tune()`. For example, for the single layer neural network used in Section \@ref(overfitting-bad), the number of hidden units is designated for tuning using:

```{r tuning-mlp-units}
neural_net_spec <- 
  mlp(hidden_units = tune()) %>% 
  set_engine("keras")
```

The `tune()` function doesn't execute any particular parameter value; it simply returns an _expression_: 

```{r tuning-tune-exprs}
tune()
```

Embedding this value in an argument will tag the parameter for optimization. The model tuning functions shown in the next two chapters parse the model specification and/or recipe to discover the tagged parameters. These functions can automatically configure and process these parameters since they understand their characteristics (e.g. the range of possible values, etc.). 

To enumerate the tuning parameters for an object, the `dials::parameters()` function is used: 

```{r tuning-mlp-units-param}
parameters(neural_net_spec)
```

The results show a value of `nparam[+]`, indicating that the number of hidden units is a _numeric_ parameter. 

There is a single optional identification argument that associates a name with the parameters. This can come in handy when the same parameter is being tuned in different places. For example, using the Ames object from Section \@ref(resampling-summary), the recipe encoded longitude and latitude with spline functions. If we want to tune the two spline functions to potentially have different levels of smoothness, `step_ns()` is called twice (once for each predictor). To make the parameters identifiable, the identification argument takes any character string: 

```{r tuning-id}
ames_rec <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
           Latitude + Longitude, data = ames_train)  %>%
  step_log(Gr_Liv_Area, base = 10) %>% 
  step_other(Neighborhood, threshold = tune()) %>% 
  step_dummy(all_nominal()) %>% 
  step_interact( ~ Gr_Liv_Area:starts_with("Bldg_Type_") ) %>% 
  step_ns(Longitude, deg_free = tune("longitude df")) %>% 
  step_ns(Latitude,  deg_free = tune("latitude df"))

recipes_param <- parameters(ames_rec)
recipes_param
```

Note that the `identifier` and `type` columns have different values.

When a recipe and model specification are combined using a workflow, both sets of parameters are shown:

```{r tuning-mlp-rec}
wflow_param <- 
  workflow() %>% 
  add_recipe(ames_rec) %>% 
  add_model(neural_net_spec) %>% 
  parameters()
wflow_param
```

:::rmdnote
Neural networks are very capable of emulating nonlinear patterns. Adding spline terms to this type of model is unnecessary; we combined this model and recipe for illustration only. 
:::

Each tuning parameter argument has a corresponding function from the `r pkg(dials)` package. In the vast majority of the cases, the function has the same name as the parameter argument: 

```{r tuning-dials-unit}
hidden_units()
threshold()
```

The `deg_free` parameter is a counter-example; the notion of degrees of freedom comes up in a variety of different situations. When used with splines, there is a specialized `r pkg(dials)` function called `spline_degree()` that is, by default, invoked: 

```{r tuning-spline-df}
spline_degree()
```

`dials` also has a convenience function for extracting a particular parameter object:

```{r tuning-extract}
# identify the parameter using the id value:
wflow_param %>% pull_dials_object("threshold")
```

Inside the parameter set, the range of the parameters can also be updated in-place: 

```{r tuning-update}
parameters(ames_rec) %>% 
  update(threshold = threshold(c(0.8, 1.0)))
```

The _parameter sets_ created by `parameters()` are consumed by the tidymodels tuning functions (when needed). If the defaults for the tuning parameter objects require modification, a modified parameter set is passed to the approriate tuning function. 

Some tuning parameters depend on the dimensions of the data. For example, the number of nearest neighbors must be between one and the number of rows in the data. In some cases, it is easy to have reasonable defaults for the range of possible values. In other cases, the parameter range is critical and cannot be assumed. The primary tuning parameter for random forest models is the number of predictor columns that are randomly sampled for each split in the tree, usually denoted as `mtry()`. Without knowing the number of predictors, this parameter range cannot be pre-configured and required finalization. 

```{r tuning-rf}
rf_spec <- 
  rand_forest(mtry = tune()) %>% 
  set_engine("ranger", regularization.factor = tune("regularization"))

rf_param <- parameters(rf_spec)
rf_param
```
 
Complete parameter objects have `[+]` in their summary; a value of `[?]` indicates that at least one end of the possible range is missing There are two methods for handling this. First, is to use `update()` to add a range based on what you know about the data dimensions:

```{r tuning-rfupdate}
rf_param %>% 
  update(mtry = mtry(c(1, 70)))
```

However, this approach might not work if a recipe is attached to the workflow that uses steps that either add or subtract columns. If those steps are not slated for tuning, the `finalize()` function can execute the recipe once to obtain the dimensions: 

```{r tuning-finalize}
pca_rec <- 
  recipe(Sale_Price ~ ., data = ames_train) %>% 
  # Select the square-footage predictors and extract their PCA components:
  step_normalize(contains("SF")) %>% 
  # Select the number of components needed to capture 95% of
  # the variance in the predictors. 
  step_pca(contains("SF"), threshold = .95)
  
udpated_param <- 
  workflow() %>% 
  add_model(rf_spec) %>% 
  add_recipe(pca_rec) %>% 
  parameters() %>% 
  finalize(ames_train)
udpated_param
udpated_param %>% pull_dials_object("mtry")
```

When the recipe is prepared, the `finalize()` learns to set the upper range of `mtry` to  `r udpated_param %>% pull_dials_object("mtry") %>% range_get() %>% pluck("upper")` predictors. 

Additionally, the results of `parameters()` will include engine-specific parameters (if any) . They are discovered in the same way as the main arguments and included in the parameter set. The `r pkg(dials)` package contains parameter functions for all potentially tuned engine-specific parameters: 

```{r tuning-engine-param-set}
rf_param
regularization_factor()
```

Finally, it should be notes that some tuning parameters are best associated with transformations. A good example of this is the penalty parameter associated with many regularized regression models. This parameter is non-negative and it is common to vary its values in log units. The primary `r pkg(dials)` parameter object indicates that a transformation is used by default: 

```{r tuning-penalty}
penalty()
```

This is important to know, especially when altering the range. New range values _must be in the transformed_ units:

```{r tuning-penalty-set}
# correct method to have penalty values between 0.1 and 1.0
penalty(c(-1, 0)) %>% value_sample(1000) %>% summary()

# incorrect:
penalty(c(0.1, 1.0)) %>% value_sample(1000) %>% summary()
```

Changing the scale is easy to do. To use natural units but the same range: 

```{r tuning-penalty-natural}
penalty(trans = NULL, range = 10^c(-10, 0))
```

## Chapter summary

This chapter described the concepts of tuning parameters and overfitting. Since overfitting can occur when the wrong tuning parameter values are used, the chapter introduced two general strategies for determining the right values. In tidymodels, the `tune()` function is used to identify the parameters for optimization. Finally, functions from the `r pkg(dials)` package illustrated how to extract and interact with tuning parameters objects.  

