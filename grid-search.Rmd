```{r grid-setup, include = FALSE}
knitr::opts_chunk$set(fig.path = "figures/")
library(tidymodels)
library(finetune)
library(ggforce)
data(cells)

library(doMC)
registerDoMC(cores = parallel::detectCores(logical = TRUE))
```


# Grid search {#grid-search}

Recall from the last chapter that users can mark arguments in recipes and/or model specifications for optimization using a value of `tune()`.  Once we know _what_ to optimize, there is the question of _how_ to optimize the parameters. This chapter describes _grid search_ methods that specify the possible values of the parameters _a priori_. We'll start by describing two main approaches for assembling a grid. 

## Regular and non-regular grids

There are two main types of grids. The first is a _regular grid_ where each parameter has a corresponding set of possible values. A regular grid combines these factorially, i.e., by using all combinations of the sets. Alternatively, a non-regular grid is one where the parameter combinations are not formed from a small set of points. 

Before considering each type, let's consider an example model: the  multilayer perceptron model (a.k.a. single layer artificial neural network).  The parameters marked for tuning are: 

* The number of hidden units. 

* The number of fitting epochs/iterations in model training.  

* The amount of weight decay penalization. 

Historically, the number of epochs was determined by early stopping; a separate validation set determined the length of training based on the error rate. It was understood that re-predicting the training set led to overfitting. In our case, the use of a weight decay penalty should prohibit overfitting. However, there is little harm in tuning the penalty and the number of epochs. 

Using `r pkg(parsnip)`, the specification for a classification model fit using the `r pkg(nnet)` package would be: 

```{r grid-mlp}
mlp_spec <- 
 mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %>% 
 set_engine("nnet", trace = 0) %>% 
 set_mode("classification")
```

The option for `trace` prevents extra logging of the training process. As previously show, the `parameters()` function can extract the set of arguments with unknown values and sets there `r pkg(dials)` objects: 

```{r grid-mlp-param}
mlp_param <- parameters(mlp_spec)
mlp_param %>% pull_dials_object("hidden_units")
mlp_param %>% pull_dials_object("penalty")
mlp_param %>% pull_dials_object("epochs")
```

This output indicates that the parameter objects are complete and prints their default ranges. These values will be used to demonstrate how to create different types of parameter grids. 

### Regular grids

As previously mentioned, regular grids are combinations of separate sets of parameter values. First, the user creates a distinct set of values for each parameter.  The number of possible values need not be the same for each parameter. For example, the `r pkg(tidyr)` function `crossing()` is one way to create a regular grid: 

```{r grid-crossing}
crossing(
  hidden_units = 1:3,
  penalty = c(0.0, 0.1),
  epochs = c(100, 200)
)
```

The parameters object knows the ranges of the parameters. The `r pkg(dials)` packages contains a set of `grid_*()` functions that take the parameters object as input to produce different types of grids. For example: 

```{r grid-reg}
grid_regular(mlp_param, levels = 2)
```
The `levels` argument is the number of levels _per parameter_ to create. It can also take a named vector of values: 

```{r grid-reg-lvls}
mlp_param %>% 
  grid_regular(levels = c(hidden_units = 3, penalty = 2, epochs = 2))
```

There are techniques for creating regular grids that do not use all possible values of each parameter set. These _fractional factorial designs_ [@BHH] could also be used. To learn more, consult the CRAN Task View for experimental design^[[https://CRAN.R-project.org/view=ExperimentalDesign](https://CRAN.R-project.org/view=ExperimentalDesign)]. 

Regular grids can be computationally expensive to use, especially when there are a medium to large number of tuning parameters. This is true for many models but not all. As discussed in Section \@ref(efficient-grids) below, there are many models who tuning time _decreases_ with a regular grid.

One advantage to using a regular grid is that the relationships and patterns between the tuning parameters and the model metrics are easily understood. The factorial nature of these designs allows for examination of each parameter separately with little confounding between parameters.   

### Irregular grids

There a several options for creating non-regular grids. The first is to use random sampling across the range of parameters. `grid_random()` generates independent uniform random numbers across the parameter ranges. If the parameter object has an associated transformation (such as we have for `penalty`), the random numbers are generated on the transformed scale.  For example: 

```{r grid-rand}
set.seed(100)
mlp_param %>% 
  grid_random(size = 1000) %>% # 'size' is the number of combinations
  summary()
```
For `penalty`, the random numbers are uniform on the log (based 10) scale but the values in the grid are in the natural units. 

The issue with random grids is that, with small-to-medium grids, random values can result in overlapping parameter combinations. Also, the random grid should cover the whole parameter space but the likelihood of this occurring increases with the number of grid values. For a sample of 20 candidate points, this plot shows some overlap between points: 

```{r grid-random-matrix, fig.height = 6, fig.width = 6}
library(ggforce)
set.seed(100)
mlp_param %>% 
  # The 'original = FALSE' option keeps penalty in log10 units
  grid_random(size = 20, original = FALSE) %>% 
  ggplot(aes(x = .panel_x, y = .panel_y)) + 
  geom_point() +
  geom_blank() +
  facet_matrix(vars(hidden_units, penalty, epochs), layer.diag = 2) + 
  labs(title = "Random design with 20 candidates")
```

A much better approach uses a set of experimental designs called _space-filling designs_. While different design methods have slightly different goals, they generally find a configuration of points that cover the parameter space with the smallest chance of overlapping or redundant values. Examples of such designs are latin hypercubes [@lhd], maximum entropy designs [@maxent], maximum projection designs [@maxproj], and others. See @santner2003design for an overview. 

`r pkg(dials)` contains functions for latin hypercube and maximum entropy designs. As with `grid_random()`, the primary inputs are the number of parameter combinations and a parameters object. Let's compare a random design with a latin hypercube design for 20 candidate parameter values. 

```{r grid-sfd-compare, fig.height = 6, fig.width = 6}
set.seed(100)
mlp_param %>% 
  grid_latin_hypercube(size = 20, original = FALSE) %>% 
  ggplot(aes(x = .panel_x, y = .panel_y)) + 
  geom_point() +
  geom_blank() +
  facet_matrix(vars(hidden_units, penalty, epochs), layer.diag = 2) + 
  labs(title = "Latin Hypercube design with 20 candidates")
```

While not perfect, this design spaces the points further away from one another.  

Space-filling designs can be very effective at representing the parameter space. As shown below, the default design used by the `r pkg(tune)` package is the maximum entropy design. These tend to produce grids that cover the candidate space well and drastically increase the changes of finding good results. 

## Evaluating the grid

The process for choosing the best tuning parameter combination is familiar: each candidate set is assessed using data that were not used to train the model. Resampling methods or a simple validation set work well for this purpose. The process (and syntax) closely resamples the approach in Section \@ref(resampling-performance) that used the `fit_resamples()` function from the `r pkg(tune)` package. 

After resampling, the use selects the most appropriate candidate parameter set. It might make sense to choose the empirically best parameter combination or bias the choice towards other aspects of the model fit, such as simplicity. 



@Hill developed a high-content laboratory method for cancer research. The data consists of 58 imaging measurements on 2019 cells. These predictors represent shape and intensity characteristics of different parts of the cells (e.g., the nucleus, the cell boundary, etc.). Each cell belongs to one of two classes.

There is a high-degree of correlation between the predictors. For example, there are several different predictors that measure the size and shape of the nucleus and cell boundary. Also, individually, many predictors have skewed distributions.


```{r grid-cells}
data(cells)
cells <- cells %>% select(-case)
```

```{r grid-cells-folds}
set.seed(33)
cell_folds <- vfold_cv(cells)
```

Given the high degree of correlation between predictors, it may be sensible to use PCA feature extraction to decorrelate the predictors. The following recipe contains steps to transform the predictors into increase symmetry, normalize them to be on the same scale then conduct feature extraction. The number of PCA components to retain should is also tuned. While the resulting PCA components are technically on the same scale, the earlier components tend to have a wider range than the later components. For this reason, a final normalization will coerce the predictors to have the same mean and variance.

```{r grid-cells-objects}
mlp_rec <-
  recipe(class ~ ., data = cells) %>%
  step_YeoJohnson(all_predictors()) %>% 
  step_normalize(all_predictors()) %>% 
  step_pca(all_predictors(), num_comp = tune()) %>% 
  step_normalize(all_predictors())

mlp_wflow <- 
  workflow() %>% 
  add_model(mlp_spec) %>% 
  add_recipe(mlp_rec)
```

A parameter object is also created to adjust a few of the default ranges. The number of epochs will have a smaller range (50 to 200 epochs). Also, the default range for `num_comp()`  defaults to a very narrow range (one to four components). The range is increase up to 40 components and the minimum value is set to zero. In `step_pca()`, using zero PCA components is a shortcut to _skip_ the feature extraction. In this way, the original predictors can be directly compared to the results that include PCA components. 

```{r grid-cells-workflow}
mlp_param <- 
  mlp_wflow %>% 
  parameters() %>% 
  update(
    epochs = epochs(c(50, 200)),
    num_comp = num_comp(c(0, 40))
  )
```

The `tune_grid()` function is the primary function for conducting grid search. Its functionality is very similar to `fit_resamples()` from Section \@ref(resampling-performance) although it has additional arguments related to the grid: 

* grid: An integer or data frame. When an integer is used, the function creates a space filling design with `grid` candidate parameters combinations. If specific parameter combinations exist, the `grid` parameter is used to pass them to the function. 

* `param_info`: An _optional_ argument for defining the parameter ranges. The is most useful when `grid` is an integer. 

Otherwise, the interface to `tune_grid()` is the same as `fit_resamples()`. The first argument is either a model specification or workflow. When a model is given, the second argument can be either a recipe or formula. The other required argument is an `r pkg(rsample)` resampling object (such as `cell_folds`). The call below also passes a metric set so that the area under the ROC curve is measured during resampling. 

To start, a regular grid with three levels is evaluated across the resamples: 

```{r grid-cells-regular, cache = TRUE}
roc_res <- metric_set(roc_auc)
set.seed(99)
mlp_reg_tune <-
  mlp_wflow %>%
  tune_grid(
    cell_folds,
    grid = mlp_param %>% grid_regular(levels = 3),
    metrics = roc_res
  )
mlp_reg_tune
```

As before, there are high-level convenience functions to understand the results. First, the `autoplot()` method for regular grids shows the performance profiles across tuning parameters:  

```{r grid-cells-reg-plot, fig.height = 6}
autoplot(mlp_reg_tune)
```

For these data, the amount of penalization has the largest impact on the area under the ROC curve. The number of epochs doesn't appear to have a pronounced effect on performance. The change in the number of hidden units appears to matter most when the amount of regularization is low (with reduced performance). There are several parameter configurations that have roughly equivalent performance, as seen using the function `show_best()`: 

```{r grid-cells-reg-best}
show_best(mlp_reg_tune) %>% select(-.estimator)
```

Based on these results, it would make sense to conduct another run of grid search with larger values of the weight decay penalty. 

To use a space-filling design, the `grid` argument can be given an integer or one of the `grid_*()` functions can produce a data frame To evaluate the same range using a maximum entropy design with 20 candidate values: 

```{r grid-cells-sdf, cache = TRUE}
set.seed(99)
mlp_sfd_tune <-
  mlp_wflow %>%
  tune_grid(
    cell_folds,
    grid = 20,
    # Pass in the parameter object to use the appropriate range: 
    param_info = mlp_param,
    metrics = roc_res
  )
mlp_sfd_tune
```

The `autoplot()` method will also work with these designs although the format of the results are different:

```{r grid-cells-sdf-plot}
autoplot(mlp_sfd_tune)
```

This _marginal effects plot_ shows the relationship of each parameter with the performance metric. Care should be taken when examining this plot; since a regular grid is not used, the values of the other tuning parameters can affect each panel. 

For example, the penalty parameter appears to have better performance with smaller amounts of weight decay. This is the opposite of the results form the regular grid. Since each point each panel is shared with the other three tuning parameters, the trends in one panel can be affected by the others. Using a regular grid, each point in each panel is equally averaged over the other parameters. For this reason, the affect of each parameter is better isolated with regular grids.  

As with the regular grid, `show_best()` can report on the numerically best results: 

```{r grid-cells-sdf-best}
show_best(mlp_sfd_tune) %>% select(-.estimator)
```

Generally, it is a good idea to evaluate the models over multiple metrics so that different aspects of the model fit are taken into account. Also, it often makes sense to choose a slightly sub-optimal parameter combination that is associated with a simpler model. For this model, simplicity corresponds to larger penalty values and/or fewer hidden units.  

As with the results from `fit_resamples()`, there is usually no value in retaining the intermediary model fits across the resamples and tuning parameters. However, as before, the `extract` option to `control_grid()` allows the retention of the fitted models and/or recipes. Also, setting the `save_pred` option to `TRUE` retains the assessment set predictions and these are easily accessed using `collect_predictions()`. 

## Finalizing the model

If this model were an attractive one for these data, we might wish to evaluate how well it does on the test set. However, the results of `tune_grid()` only provide the substrate to choose appropriate tuning parameter. The function does not fit a final model. 

To do this, a set of parameter values must be determined. There are tow methods to do so: manually picking values that appear appropriate or to use a `select_*()` function. 

Looking at the results for the regular grid plot, a model with a single hidden unit trained for 125 epochs on the original predictors with a large amount of penalization has competitive performance. It is basically penalized logistic regression. To manually specific these parameters, a tibble is created with these values and a _finalization_ function splices the values back into the workflow: 

```{r grid-finalize-manual}
logistic_param <- 
  tibble(
    num_comp = 0,
    epochs = 125,
    hidden_units = 1,
    penalty = 1
  )

final_mlp_wflow <- 
  mlp_wflow %>% 
  finalize_workflow(logistic_param)
final_mlp_wflow
```
No residual values of `tune()` are included in the workflow. Now the model is fit to the _entire training set_: 

```{r grid-final-model}
final_mlp_fit <- 
  final_mlp_wflow %>% 
  fit(cells)
```

This object is used to make future predictions. 

## Tools for efficient grid search {#efficient-grids}

parallelism, sub-models, and racing  


