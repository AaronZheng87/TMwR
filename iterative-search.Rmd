```{r iterative-setup, include = FALSE}
knitr::opts_chunk$set(fig.path = "figures/")
library(tidymodels)
library(finetune)
library(patchwork)
library(ragg)
library(doMC)
registerDoMC(cores = parallel::detectCores(logical = TRUE))

## -----------------------------------------------------------------------------

source("extras/sa_2d_plot.R")
source("extras/bo_3panel_plot.R")
load(file.path("RData", "svm_large.RData"))

## -----------------------------------------------------------------------------

data(cells)
cells <- cells %>% select(-case)
set.seed(33)
cell_folds <- vfold_cv(cells)
roc_res <- metric_set(roc_auc)
```

# Iterative search {#iterative-search}



## A support vector machine model

The cell data, used in the previous chapter, are also used here. We will use a support vector machine (SVM) model to demonstrate sequential tuning methods. See @apm for model information on this model. The two tuning parameters that will be optimized are the SVM cost value as well has the radial basis function kernel parameter $\sigma$. Both parameters can have a profound effect on the model complexity and performance. 

Like the  multilayer perceptron model,  this model would benefit from the use of the PCA feature extraction. However, this operation is not used in this chapter so that we can visualize the search process in two dimensions. The support vector machine model uses a dot-product and, for this reason, centering and scaling is applied to the predictors. 

Along with the previously used objects (shown in Section \@ref(grid-summary)), the following tidymodels objects define the model process: 

```{r iterative-svm-defs}
svm_rec <- 
  recipe(class ~ ., data = cells) %>%
  step_YeoJohnson(all_predictors()) %>%
  step_normalize(all_predictors())

svm_spec <- 
 svm_rbf(cost = tune(), rbf_sigma = tune()) %>% 
 set_engine("kernlab") %>% 
 set_mode("classification")

svm_wflow <- 
  workflow() %>% 
  add_model(svm_spec) %>% 
  add_recipe(svm_rec)
```

The default parameter ranges for these two tuning parameters are: 

```{r iterative-svm-param}
cost()
rbf_sigma()
```

For illustration, the parameter ranges are slightly changed (to improve the visualizations of the search): 

```{r iterative-svm-param-set}
svm_param <- 
 svm_wflow %>% 
 parameters() %>% 
  update(
    cost = cost(c(-10, 5)),
    rbf_sigma = rbf_sigma(c(-7, -1))
  )
```

Before discussing the search procedures, it is helpful to understand what the relationship is between the tuning parameters and the area under the ROC curve. A very large regular grid, comprised of 2,500 candidate values, was constructed and evaluated using resampling. This is obviously impractical in regular data analysis and tremendously inefficient. However, it elucidates the path that the search process _should take_ and where the numerically optimal value(s) occur. 

The plot below shows the results. There is a large swath in the lower diagonal of the parameter space that is relatively flat with poor performance. A ridge of best performance occurs in the upper right portion of the space. The transition from the plateau of poor results to the ridge of best performance is very sharp. The is also a sharp drop in the area under the ROC curve just to the right of the ridge. A black dot points to the numerically best results found in the grid. 

```{r iterative-roc-surface, out.width = "80%", echo = FALSE, warning = FALSE}
# See file extras/cells_svm_large.R
res <- file.copy("premade/roc_surface.png", "_book/premade/roc_surface.png")
knitr::include_graphics("premade/roc_surface.png")
```

The search procedures described below require at least one tuning parameter result before proceeding. For this purpose, the code below creates a small regular grid that resides in the flat portion of the parameter space. The `tune_grid()` function resamples these values:  

```{r iterative-svm-initial, cache = TRUE}
set.seed(234)
start_grid <- 
 svm_param %>% 
 update(
  cost = cost(c(-6, 1)),
  rbf_sigma = rbf_sigma(c(-6, -4))
 ) %>% 
 grid_regular(levels = 2)

set.seed(2)
svm_initial <- 
  svm_wflow %>% 
  tune_grid(resamples = cell_folds, grid = start_grid, metrics = roc_res)

collect_metrics(svm_initial)
```
As shown in the following sections, these results can be ingested by the iterative tuning functions to use as initial values. 

## Bayesian  optimization

Bayesian optimization techniques analyze the current resampling results and create a predictive model to predict tuning parameter values that have yet to be evaluated. The tuning parameter combination estimated to be most advantageous is then resampled. These results are then used in another predictive model that predicts more candidate values for testing. The process proceeds for a set number of iterations or until no further improvements occur. 

The main questions in this tuning scheme are how to create the model and how to select parameters predicted by that model. 

### A Gaussian process model




### Acquisition functions



### The `tune_bayes()` function

```{r iterative-cells-bo-calcs, echo = FALSE, cache = TRUE}
# We will do the calculations here but use some non-standard options. First, 
# purrr is used to capture the output in a vector so that we can show the 
# results piecemeal. Also, a hidden option is used to save the grid of candidate 
# values for each iteration of the search. These will be used to make an 
# animation in a later chunk. 
# 
# This means that any changes to this chunk have to be made to the next chunk 
# (where the code is shown and not executed).

ctrl <- control_bayes(verbose = TRUE)
ctrl$save_gp_results <- TRUE

tune_bayes_sssshhh <- purrr::quietly(tune_bayes)

set.seed(1234)
svm_bo_sshh <-
  svm_wflow %>%
  tune_bayes_sssshhh(
    resamples = cell_folds,
    metrics = roc_res,
    initial = svm_initial,
    param_info = svm_param,
    iter = 25,
    control = ctrl
  )

svm_bo <- svm_bo_sshh$result
svm_bo_output <- svm_bo_sshh$messages

gp_candidates <- collect_gp_results(svm_bo)
save(gp_candidates, file = "RData/gp_candidates.RData")
```

```{r iterative-cells-bo, eval = FALSE}
ctrl <- control_bayes(verbose = TRUE)

set.seed(1234)
svm_bo <-
 svm_wflow %>%
 tune_bayes(
  resamples = cell_folds,
  metrics = roc_res,
  initial = svm_initial,
  param_info = svm_param,
  iter = 25,
  control = ctrl
 )
```


```{r iterative-cells-info, include = FALSE}
bo_res <- collect_metrics(svm_bo) %>% mutate(current_best = FALSE)
for(i in 1:nrow(bo_res)) {
  bo_res$current_best[i] <- bo_res$mean[i] > max(bo_res$mean[1:(i-1)])
}
init_vals <- bo_res %>% dplyr::filter(.iter == 0)
best_init <- max(bo_res$mean[bo_res$.iter == 0])
best_bo <- max(bo_res$mean)
best_bo_iter <- bo_res$.iter[which.max(bo_res$mean)]
new_best_iter <- bo_res$.iter[which(bo_res$current_best)]
new_best_iter <- new_best_iter[new_best_iter > 0]
num_improve <- length(new_best_iter)
last_iter <- max(collect_metrics(svm_bo)$.iter)

iter_1_roc <- bo_res$mean[bo_res$.iter == 1]
iter_1_imp <- iter_1_roc > best_init
iter_1_text <- 
  paste0(
    ifelse(iter_1_imp, "showed an improvement, resulting in an ROC value of ",
                        "failed to improve the outcome with an ROC value of "),
    round(iter_1_roc, 5), "."
  )

iter_2_roc <- bo_res$mean[bo_res$.iter == 2]
iter_2_imp <- iter_2_roc > max(bo_res$mean[bo_res$.iter < 2])
iter_2_text <- 
  dplyr::case_when(
    !iter_1_imp &  !iter_2_imp ~ 
      paste0("the second iteration also failed to yield an improvement."),
    !iter_1_imp &   iter_2_imp ~ 
      paste0("the second iteration did yield a better result with an area under the ROC curve of ", 
             round(iter_2_roc, 5), "."),
     iter_1_imp &  !iter_2_imp ~ 
      paste0("the second iteration did not continue the trend with a sub-optimal ROC value of ",
             round(iter_2_roc, 5), "."),
      iter_1_imp &  !iter_2_imp ~ 
      paste0("the second iteration further increased the outcome value (ROC = ",
             round(iter_2_roc, 5), ").") 
  )

if (num_improve > 1) {
  improve_text <-
    paste0(
      "There were a total of ",
      num_improve,
      " improvements in the outcome along the way at iterations ",
      knitr::combine_words(new_best_iter),
      "."
    )
} else {
  improve_text <-
    paste0("There was only a single improvement in the outcome at iteration ",
           new_best_iter,
           ".")
}

if (last_iter < 25) {
  last_bo_text <-
    paste0(
      "Since the best result occurred at iteration ",
      max(new_best_iter),
      ", there were no more improvements and the default option is to stop if no improvements are found after ",
      ctrl$no_improve,
      " more steps. The last step was:"
    )
} else {
    last_bo_text <- "The last step was:"
}
```

The search process starts with an initial value of `r round(best_init, 5)` for the area under the ROC curve. A Gaussian process model uses these `r nrow(init_vals)` statistics to create a model. The large candidate set is automatically generated and scored using the expected improvement acquisition function. The first iteration `r iter_1_text` After fitting another Gaussian process model with the new outcome value, `r iter_2_text`

The log of the first two iterations, produced by the `verbose` option, was: 

```{r iterative-cells-bo-print-first, echo = FALSE}
so_stop_index <- grep("Iteration 3", svm_bo_output) 
if (length(so_stop_index) > 0) {
  cat(svm_bo_output[1:(so_stop_index - 2)], sep = "")
}
```
The search continues. `r improve_text` The best result occurred at iteration `r max(new_best_iter)` with an area under the ROC curve of `r round(best_bo, 5)`. 

```{r iterative-cells-bo-print-impr, echo = FALSE}
all_imp_index <- grep("â™¥", svm_bo_output)
so_stop_index <- all_imp_index[length(all_imp_index)]
if (length(all_imp_index) > 0) {
  so_start_index <- so_stop_index - 10
  cat(svm_bo_output[so_start_index:(so_stop_index + 1)], sep = "")
}
```
`r last_bo_text` 

```{r iterative-cells-bo-print-last, echo = FALSE}
so_start <- paste("Iteration", last_iter)
so_start_index <- grep(so_start, svm_bo_output)
if (length(so_start_index) > 0) {
  cat(svm_bo_output[so_start_index:length(svm_bo_output)], sep = "")
}
```

The `autoplot()` function has several options for iterative search methods. One shows how the outcome changed over the search: 

```{r iterative-bo-iter, fig.height = 4}
autoplot(svm_bo, type = "performance")
```

The animation below visualizes the results of the search. The black X values show the starting values contained in `svm_initial`. The top-left blue panel shows the _predicted_ mean value of the area under the ROC curve. The red panel on the top-right displays the _predicted_ variation in the ROC values while the bottom plot visualizes the expected improvement. In each panel, darker colors indicate less attractive values (e.g., small mean values, large variation, and small improvements).  

```{r iterative-bo-progress, out.width = "100%", echo = FALSE, warning = FALSE}
# If caching was turned on for the 'iterative-cells-bo-calcs' chunk, the version
# of the GP results in tempdir will not be available so we use the last copy in 
# the 'RData' path. 

if (!exists("gp_candidates")) {
  load("RData/gp_candidates.RData")
}
res <- make_bo_animation(gp_candidates, svm_bo)
knitr::include_graphics("_book/premade/bo_search.gif")
```

The surface of the predicted mean is very inaccurate in the first few iterations of the search. Despite this, it does help guide the process to the region of good performance.  In other words, the Gaussian process model is wrong but shows itself to be very useful. Within the first ten iterations the search is sampling near the optimum location. 

While the best tuning parameter combination is on the boundary of the parameter space, Bayesian optimization will often choose new points on other parts of the boundary). While we can adjust the ratio of exploration and exploitation, the search tends to sample boundary points.

If the search is seeded with an initial grid, a space-filling design would probably be a better choice than a regular design.  It samples more unique values of the parameter space and would improve the predictions of the standard deviation in the early iterations. 

`tune_bayes()` also has a somewhat non-standard option in its control function. In cases where the search is no exploring the space enough, it take an _uncertainty sample_ if there have been too few improvements in the outcome. This type of sample would ignore the predicted mean and choose a new candidate that has the largest uncertainty. The result is that the search might explore a location where there are no existing points. This idea comes from the field of active learning, where new training set points are chosen based on their ability to improve the model fit. To use this feature, the `uncertain` argument in `control_bayes()` specifies how many steps with no improvement must occur before taking an uncertainty sample. The default is to never take one. 

Finally, if the user interrupts the `tune_bayes()` computations, the function returns the current results (instead of resulting in an error).  


## Simulated annealing 

```{r iterative-cells-sa-calcs, include = FALSE, cache = TRUE}
# As we did above, this chunk executes the code with some extra options that
# capture the output and save some internal objects for plotting. 

# This means that any changes to this chunk have to be made to the next chunk 
# (where the code is shown and not executed).

ctrl_sa <- control_sim_anneal(no_improve = Inf)
ctrl_sa$sa_history <- TRUE

tune_sim_anneal_sssshhh <- purrr::quietly(tune_sim_anneal)

set.seed(1234)
svm_sa_sshh <-
  svm_wflow %>%
  tune_sim_anneal_sssshhh(
    resamples = cell_folds,
    metrics = roc_res,
    initial = svm_initial,
    param_info = svm_param,
    iter = 50,
    control = ctrl_sa
  )

svm_sa <- svm_sa_sshh$result
svm_sa_output <- svm_sa_sshh$messages

# We set tune_sim_anneal() to save a file to the temp directory. If the results
# are cached, this file won't be there:
try(
  file.copy(
    file.path(tempdir(), "sa_history.RData"),
    "RData/sa_history.RData"
  ),
  silent = TRUE
)
```
```{r iterative-cells-sa, eval = FALSE}
ctrl_sa <- control_sim_anneal(no_improve = Inf)

set.seed(1234)
svm_sa <-
  svm_wflow %>%
  tune_sim_anneal(
    resamples = cell_folds,
    metrics = roc_res,
    initial = svm_initial,
    param_info = svm_param,
    iter = 50,
    control = ctrl_sa
  )
```



```{r iterative-sa-history, include = FALSE}
if (file.exists(file.path(tempdir(), "sa_history.RData"))) {
  load(file.path(tempdir(), "sa_history.RData"))
} else {
  load("RData/sa_history.RData")
}

result_history <- get_new_best(result_history)
result_history <- get_parent(result_history)

## -----------------------------------------------------------------------------

restart_iter <- result_history$.iter[result_history$results == "restart"]
restart_num <- length(restart_iter)
sa_iter_list <- knitr::combine_words(restart_iter)
restart_txt <- 
  dplyr::case_when(
    restart_num == 0 ~ paste0("There were no restarts during the search."),
    restart_num == 1 ~ paste0("There was a single restart at iteration ", restart_iter)[1],
    TRUE ~ paste0("There were ", restart_num, " restarts at iterations ", sa_iter_list)[1]
  )
discard_num<- length(result_history$.iter[result_history$results == "discard"])
if (discard_num > 0) {
  restart_txt <-
    paste0(
      restart_txt, 
      " and ", 
      discard_num, 
      " discarded ", 
      ifelse(discard_num > 1, "candidates ", "candidate "),
      "during the process."
    )
} else {
  restart_txt <- paste0(restart_txt, ".")
}

## -----------------------------------------------------------------------------

best_iters <- result_history$.iter[result_history$new_best]
best_init <- max(result_history$mean[result_history$.iter == 0])
best_sa_res <- max(result_history$mean[result_history$.iter > 0])
best_sa_inds <- result_history$.iter[which.max(result_history$mean)]
best_txt <-
  dplyr::case_when(
    restart_num == 1 ~ paste0("a new global optimum once at iteration ", best_iters, "."),
    TRUE ~ paste0("new global optimums at ", length(best_iters), " different iterations.")[1]
  )
best_txt <- best_txt[1]
if (length(best_iters) > 1) {
  best_txt <-
    paste0(
      best_txt,
      " The earliest improvement was at iteration ",
      min(best_iters),
      " and the final optimum occured at iteration ",
      max(best_iters),
      ". The best overall results occured at iteration ", 
      best_sa_inds, " with a mean area under the ROC curve of ",
      round(best_sa_res, 4), " (compared to an initial best of ",
      round(best_init, 4), ")."
    )
}
```

The simulated annealing process discovered `r best_txt` `r restart_txt`

The `verbose` option prints details of the search process. The output for the first five iterations was: 

```{r iterative-cells-sa-print-start, echo = FALSE}
so_stop_index <- grep("^ 5", svm_sa_output)
if (length(so_stop_index) > 0) {
  cat(svm_sa_output[1:so_stop_index], sep = "")
}
```


The last ten iterations: 

```{r iterative-cells-sa-print-end, echo = FALSE}
so_start_index <- grep("^40", svm_sa_output)
so_stop_index <- grep("^50", svm_sa_output)
if (length(so_stop_index) > 0) {
  cat(svm_sa_output[so_start_index:so_stop_index], sep = "")
}
```

As with the other `tune_*()` functions, the corresponding `autoplot()` function produces visual assessments of the results: 

```{r interative-sa-performance, fig.height = 4.25}
autoplot(svm_sa, type = "performance")
```

```{r interative-sa-parameters, fig.height = 4.25}
autoplot(svm_sa)
```

A visualization of the search path helps understand where the search process did well and and where it went astray:  

```{r iterative-sa-plot, echo = FALSE, out.width = '100%', warning = FALSE}
sa_gif_path <- sa_2d_plot(svm_sa, result_history, svm_large)
copy_res <- file.copy(sa_gif_path,  "_book/premade/sa_search.gif", overwrite = TRUE)
knitr::include_graphics("_book/premade/sa_search.gif")
```


## Chapter summary {#iterative-summary}
